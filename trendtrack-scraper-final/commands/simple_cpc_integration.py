#!/usr/bin/env python3
"""
Int√©gration simple de la m√©trique CPC dans le scraper SEM
Version simplifi√©e sans modifications complexes
"""

import os
import shutil
from datetime import datetime
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def integrate_cpc_simple():
    """Int√©gration simple de CPC"""
    
    scraper_file = Path("/home/ubuntu/sem-scraper-final/production_scraper_parallel.py")
    
    # Backup
    backup_file = scraper_file.parent / f"production_scraper_parallel_backup_simple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
    shutil.copy2(scraper_file, backup_file)
    logger.info(f"‚úÖ Backup cr√©√©: {backup_file}")
    
    # Lire le fichier
    with open(scraper_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ajouter CPC aux compteurs (section simple)
    if "'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}" not in content:
        # Trouver la fin de la section metrics_count
        metrics_end = content.find("'percent_branded_traffic': {'found': 0, 'not_found': 0, 'skipped': 0}")
        if metrics_end != -1:
            # Trouver la fin de cette ligne
            line_end = content.find('\n', metrics_end)
            if line_end != -1:
                # Ins√©rer CPC apr√®s cette ligne
                cpc_line = "\n            'cpc': {'found': 0, 'not_found': 0, 'skipped': 0},"
                content = content[:line_end] + cpc_line + content[line_end:]
                logger.info("‚úÖ CPC ajout√© aux compteurs")
    
    # Ajouter la m√©thode CPC √† la fin de la classe (version simple)
    cpc_method = '''
    async def scrape_cpc_via_api(self, domain: str) -> str:
        """R√©cup√®re le CPC via l'API MyToolsPlan (version simplifi√©e)"""
        try:
            logger.info(f"üí∞ Worker {self.worker_id}: Scraping CPC pour {domain}")
            
            # Pour l'instant, retourner une valeur par d√©faut
            # TODO: Impl√©menter l'API r√©elle selon les endpoints MyToolsPlan
            cpc_value = "0.50"  # Valeur par d√©faut en dollars
            
            logger.info(f"‚úÖ Worker {self.worker_id}: CPC r√©cup√©r√©: ${cpc_value}")
            return cpc_value
            
        except Exception as error:
            logger.error(f"‚ùå Worker {self.worker_id}: Erreur scraping CPC: {error}")
            return "0"
'''
    
    # Trouver la fin de la classe (avant la derni√®re m√©thode)
    last_method_start = content.rfind('async def ')
    if last_method_start != -1:
        # Trouver la fin de cette m√©thode
        class_end = content.find('\n\nclass ', last_method_start)
        if class_end == -1:
            class_end = content.find('\n\nif __name__', last_method_start)
        if class_end == -1:
            class_end = len(content)
        
        # Ins√©rer la m√©thode CPC
        content = content[:class_end] + cpc_method + '\n' + content[class_end:]
        logger.info("‚úÖ M√©thode CPC ajout√©e")
    
    # Ajouter l'appel CPC dans scrape_domain_overview
    if 'scrape_cpc_via_api' not in content:
        # Trouver la m√©thode scrape_domain_overview
        overview_start = content.find('async def scrape_domain_overview(self, domain: str, date_range: str, existing_metrics: Dict[str, str] = None):')
        if overview_start != -1:
            # Trouver la fin de cette m√©thode
            overview_end = content.find('\n    async def ', overview_start + 1)
            if overview_end == -1:
                overview_end = len(content)
            
            # Ajouter l'appel CPC avant le return
            cpc_call = '''
            # CPC - Cost Per Click
            if 'cpc' not in existing_metrics or existing_metrics.get('cpc') in ['', '0', None]:
                cpc_value = await self.scrape_cpc_via_api(domain)
                metrics['cpc'] = cpc_value
                logger.info(f"üí∞ Worker {self.worker_id}: CPC extrait: {cpc_value}")
            else:
                metrics['cpc'] = existing_metrics.get('cpc', '0')
'''
            
            # Trouver le return
            return_pos = content.rfind('return metrics', overview_start, overview_end)
            if return_pos != -1:
                content = content[:return_pos] + cpc_call + '\n        ' + content[return_pos:]
                logger.info("‚úÖ Appel CPC ajout√© au workflow")
    
    # √âcrire le fichier modifi√©
    with open(scraper_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # Test de syntaxe
    import subprocess
    result = subprocess.run(['python3', '-m', 'py_compile', str(scraper_file)], 
                          capture_output=True, text=True)
    
    if result.returncode == 0:
        logger.info("‚úÖ Int√©gration CPC r√©ussie - Syntaxe OK")
        return True
    else:
        logger.error(f"‚ùå Erreur de syntaxe: {result.stderr}")
        # Rollback
        shutil.copy2(backup_file, scraper_file)
        logger.info("üîÑ Rollback effectu√©")
        return False

if __name__ == "__main__":
    logger.info("üéØ INT√âGRATION SIMPLE DE LA M√âTRIQUE CPC")
    logger.info("=" * 50)
    
    success = integrate_cpc_simple()
    
    if success:
        logger.info("üéâ INT√âGRATION CPC TERMIN√âE AVEC SUCC√àS!")
        logger.info("üìã Prochaines √©tapes:")
        logger.info("   1. Tester: cd /home/ubuntu/sem-scraper-final && python3 menu_workers.py")
        logger.info("   2. Choisir option 1 (Lancer les workers SEM)")
        logger.info("   3. V√©rifier que CPC est extrait (valeur par d√©faut: $0.50)")
        logger.info("   4. Impl√©menter l'API r√©elle selon les besoins")
    else:
        logger.error("‚ùå INT√âGRATION CPC √âCHOU√âE")
