#!/usr/bin/env python3
"""
Int√©gration finale CPC - Version ultra-simple et s√ªre
"""

import shutil
from datetime import datetime
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def integrate_cpc_final():
    """Int√©gration finale et s√ªre de CPC"""
    
    scraper_file = Path("/home/ubuntu/sem-scraper-final/production_scraper_parallel.py")
    
    # Backup
    backup_file = scraper_file.parent / f"production_scraper_parallel_backup_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
    shutil.copy2(scraper_file, backup_file)
    logger.info(f"‚úÖ Backup cr√©√©: {backup_file}")
    
    # Lire le fichier
    with open(scraper_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # 1. Ajouter CPC aux compteurs (remplacement simple)
    if "'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}" not in content:
        old_line = "            'percent_branded_traffic': {'found': 0, 'not_found': 0, 'skipped': 0}"
        new_line = "            'percent_branded_traffic': {'found': 0, 'not_found': 0, 'skipped': 0},\n            'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}"
        
        if old_line in content:
            content = content.replace(old_line, new_line)
            logger.info("‚úÖ CPC ajout√© aux compteurs")
    
    # 2. Ajouter la m√©thode CPC √† la fin de la classe (avant les imports)
    if 'async def scrape_cpc_via_api' not in content:
        # Trouver la fin de la classe
        class_end = content.find("if __name__ == '__main__':")
        if class_end != -1:
            cpc_method = '''
    async def scrape_cpc_via_api(self, domain: str) -> str:
        """R√©cup√®re le CPC via l'API MyToolsPlan (version test)"""
        try:
            logger.info(f"üí∞ Worker {self.worker_id}: Scraping CPC pour {domain}")
            
            # Valeur par d√©faut pour test - TODO: Impl√©menter API r√©elle
            cpc_value = "2.50"
            
            logger.info(f"‚úÖ Worker {self.worker_id}: CPC r√©cup√©r√©: ${cpc_value}")
            return cpc_value
            
        except Exception as error:
            logger.error(f"‚ùå Worker {self.worker_id}: Erreur scraping CPC: {error}")
            return "0"
'''
            
            content = content[:class_end] + cpc_method + '\n\n' + content[class_end:]
            logger.info("‚úÖ M√©thode CPC ajout√©e")
    
    # 3. Ajouter un appel CPC simple dans scrape_domain_overview
    if 'cpc_value = await self.scrape_cpc_via_api' not in content:
        # Trouver un endroit pour ajouter l'appel CPC
        metrics_section = "metrics['scraping_status'] = 'completed'"
        if metrics_section in content:
            cpc_call = '''
        # CPC - Cost Per Click (test)
        cpc_value = await self.scrape_cpc_via_api(domain)
        metrics['cpc'] = cpc_value
        logger.info(f"üí∞ Worker {self.worker_id}: CPC ajout√©: {cpc_value}")
        
        # '''
            
            content = content.replace(metrics_section, cpc_call + metrics_section)
            logger.info("‚úÖ Appel CPC ajout√©")
    
    # √âcrire le fichier modifi√©
    with open(scraper_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # Test de syntaxe
    import subprocess
    result = subprocess.run(['python3', '-m', 'py_compile', str(scraper_file)], 
                          capture_output=True, text=True)
    
    if result.returncode == 0:
        logger.info("‚úÖ Int√©gration CPC r√©ussie - Syntaxe OK")
        
        # Test de validation suppl√©mentaire
        with open(scraper_file, 'r', encoding='utf-8') as f:
            content_check = f.read()
        
        checks = [
            'async def scrape_cpc_via_api' in content_check,
            'cpc_value = await self.scrape_cpc_via_api' in content_check,
            "'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}" in content_check
        ]
        
        if all(checks):
            logger.info("‚úÖ Toutes les validations CPC r√©ussies")
            return True
        else:
            logger.error("‚ùå Validations CPC √©chou√©es")
            return False
    else:
        logger.error(f"‚ùå Erreur de syntaxe: {result.stderr}")
        # Rollback
        shutil.copy2(backup_file, scraper_file)
        logger.info("üîÑ Rollback effectu√©")
        return False

if __name__ == "__main__":
    logger.info("üéØ INT√âGRATION FINALE DE LA M√âTRIQUE CPC")
    logger.info("=" * 50)
    
    success = integrate_cpc_final()
    
    if success:
        logger.info("üéâ INT√âGRATION CPC TERMIN√âE AVEC SUCC√àS!")
        logger.info("")
        logger.info("üìã R√âSUM√â DE L'INT√âGRATION:")
        logger.info("   ‚úÖ M√©thode scrape_cpc_via_api ajout√©e")
        logger.info("   ‚úÖ CPC int√©gr√© dans scrape_domain_overview")
        logger.info("   ‚úÖ CPC ajout√© aux compteurs de m√©triques")
        logger.info("   ‚úÖ Valeur par d√©faut: $2.50 (pour test)")
        logger.info("   ‚úÖ Syntaxe Python valid√©e")
        logger.info("")
        logger.info("üìã PROCHAINES √âTAPES:")
        logger.info("   1. Tester le scraper:")
        logger.info("      cd /home/ubuntu/sem-scraper-final")
        logger.info("      python3 menu_workers.py")
        logger.info("   2. Choisir option 1 (Lancer les workers SEM)")
        logger.info("   3. V√©rifier que CPC est extrait (valeur: $2.50)")
        logger.info("   4. V√©rifier en base de donn√©es:")
        logger.info("      sqlite3 /home/ubuntu/trendtrack-scraper-final/data/trendtrack.db")
        logger.info("      SELECT shop_name, cpc FROM analytics WHERE cpc IS NOT NULL LIMIT 5;")
        logger.info("")
        logger.info("üîß D√âVELOPPEMENT FUTUR:")
        logger.info("   - Impl√©menter l'API r√©elle MyToolsPlan pour CPC")
        logger.info("   - Adapter selon les endpoints disponibles")
        logger.info("   - Tester avec des donn√©es r√©elles")
    else:
        logger.error("‚ùå INT√âGRATION CPC √âCHOU√âE")
        logger.info("üîÑ Rollback effectu√© automatiquement")
