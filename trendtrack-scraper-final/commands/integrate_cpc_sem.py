#!/usr/bin/env python3
"""
Int√©gration de la m√©trique CPC dans le scraper SEM
Script d'int√©gration automatis√©e pour ajouter CPC au scraper MyToolsPlan
"""

import os
import sys
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CPCIntegration:
    """Int√©gration de la m√©trique CPC dans le scraper SEM"""
    
    def __init__(self):
        self.sem_scraper_path = Path("/home/ubuntu/sem-scraper-final")
        self.trendtrack_db_path = Path("/home/ubuntu/trendtrack-scraper-final/data/trendtrack.db")
        self.production_scraper_file = self.sem_scraper_path / "production_scraper_parallel.py"
        self.backup_dir = self.sem_scraper_path / "backup_cpc_integration"
        
    def integrate_cpc_metric(self):
        """Int√®gre la m√©trique CPC dans le scraper SEM"""
        try:
            logger.info("üöÄ D√©but de l'int√©gration de la m√©trique CPC")
            
            # √âTAPE 1: V√©rifications pr√©liminaires
            if not self._step1_verifications():
                return False
            
            # √âTAPE 2: Backup du scraper existant
            if not self._step2_backup():
                return False
            
            # √âTAPE 3: Int√©gration de la m√©thode CPC
            if not self._step3_integrate_cpc_method():
                return False
            
            # √âTAPE 4: Int√©gration dans le workflow principal
            if not self._step4_integrate_workflow():
                return False
            
            # √âTAPE 5: Mise √† jour des compteurs
            if not self._step5_update_counters():
                return False
            
            # √âTAPE 6: Test de validation
            if not self._step6_validation():
                return False
            
            logger.info("‚úÖ Int√©gration de la m√©trique CPC termin√©e avec succ√®s!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'int√©gration CPC: {e}")
            self._rollback()
            return False
    
    def _step1_verifications(self) -> bool:
        """√âTAPE 1: V√©rifications pr√©liminaires"""
        logger.info("üîç √âTAPE 1: V√©rifications pr√©liminaires")
        
        # V√©rifier que le fichier scraper existe
        if not self.production_scraper_file.exists():
            logger.error(f"‚ùå Fichier scraper non trouv√©: {self.production_scraper_file}")
            return False
        
        # V√©rifier que la base de donn√©es existe
        if not self.trendtrack_db_path.exists():
            logger.error(f"‚ùå Base de donn√©es non trouv√©e: {self.trendtrack_db_path}")
            return False
        
        # V√©rifier que la colonne CPC existe en base
        import sqlite3
        try:
            conn = sqlite3.connect(self.trendtrack_db_path)
            cursor = conn.cursor()
            cursor.execute("PRAGMA table_info(analytics)")
            columns = [row[1] for row in cursor.fetchall()]
            conn.close()
            
            if 'cpc' not in columns:
                logger.error("‚ùå Colonne 'cpc' non trouv√©e dans la table analytics")
                return False
            
            logger.info("‚úÖ Colonne 'cpc' v√©rifi√©e en base de donn√©es")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification base de donn√©es: {e}")
            return False
        
        logger.info("‚úÖ V√©rifications pr√©liminaires r√©ussies")
        return True
    
    def _step2_backup(self) -> bool:
        """√âTAPE 2: Backup du scraper existant"""
        logger.info("üíæ √âTAPE 2: Backup du scraper existant")
        
        try:
            # Cr√©er le r√©pertoire de backup
            self.backup_dir.mkdir(exist_ok=True)
            
            # Backup du fichier principal
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"production_scraper_parallel_backup_{timestamp}.py"
            shutil.copy2(self.production_scraper_file, backup_file)
            
            logger.info(f"‚úÖ Backup cr√©√©: {backup_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du backup: {e}")
            return False
    
    def _step3_integrate_cpc_method(self) -> bool:
        """√âTAPE 3: Int√©gration de la m√©thode CPC"""
        logger.info("üîß √âTAPE 3: Int√©gration de la m√©thode CPC")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # M√©thode CPC √† ajouter
            cpc_method = '''
    async def scrape_cpc_via_api(self, domain: str) -> str:
        """
        R√©cup√®re le CPC (Cost Per Click) via l'API MyToolsPlan
        Utilise l'API pour extraire les donn√©es de co√ªt par clic
        """
        try:
            logger.info(f"üí∞ Worker {self.worker_id}: Scraping CPC pour {domain}")
            
            # Throttling avant appel API
            await stealth_system.throttle_api_call(self.worker_id, "cpc")
            
            # Nettoyer le domaine
            clean_domain = domain.replace('https://', '').replace('http://', '').replace('www.', '').strip('/')
            
            # Navigation vers sam.mytoolsplan.xyz pour les appels API
            await self.page.goto("https://sam.mytoolsplan.xyz/analytics/overview/", wait_until='domcontentloaded', timeout=30000)
            await stealth_system.human_pause(self.worker_id, "session")
            
            # Appel API pour r√©cup√©rer le CPC
            # Note: √Ä adapter selon l'endpoint API MyToolsPlan pour CPC
            stealth_headers = stealth_system.get_stealth_headers()
            
            # Exemple d'appel API (√† adapter selon l'API r√©elle)
            fetch_code = f"""
                async () => {{
                    try {{
                        const response = await fetch('/dpa/rpc', {{
                            method: 'POST',
                            headers: {{
                                'Content-Type': 'application/json',
                                'User-Agent': '{stealth_headers.get('User-Agent', '')}',
                                'Accept-Language': '{stealth_headers.get('Accept-Language', '')}',
                                'Accept': '{stealth_headers.get('Accept', '')}'
                            }},
                            body: JSON.stringify({{
                                "jsonrpc": "2.0",
                                "id": 1,
                                "method": "cpc.getData",
                                "params": {{
                                    "domain": "{clean_domain}",
                                    "date": "{self.target_date}"
                                }}
                            }})
                        }});
                        
                        if (!response.ok) {{
                            throw new Error(`HTTP ${{response.status}}: ${{response.statusText}}`);
                        }}
                        
                        const data = await response.json();
                        return data;
                        
                    }} catch (error) {{
                        console.error('Erreur API CPC:', error);
                        return null;
                    }}
                }}
            """
            
            # Ex√©cuter l'appel API
            result = await self.page.evaluate(fetch_code)
            
            if result and result.get('result'):
                cpc_data = result['result']
                # Extraire la valeur CPC (√† adapter selon la structure de r√©ponse)
                cpc_value = cpc_data.get('cpc', cpc_data.get('cost_per_click', '0'))
                
                # Convertir en format num√©rique si n√©cessaire
                try:
                    cpc_numeric = float(str(cpc_value).replace('$', '').replace(',', ''))
                    logger.info(f"‚úÖ Worker {self.worker_id}: CPC r√©cup√©r√©: ${cpc_numeric:.2f}")
                    return str(cpc_numeric)
                except (ValueError, TypeError):
                    logger.warning(f"‚ö†Ô∏è Worker {self.worker_id}: CPC non num√©rique: {cpc_value}")
                    return '0'
            else:
                logger.warning(f"‚ö†Ô∏è Worker {self.worker_id}: Pas de donn√©es CPC pour {domain}")
                return '0'
                
        except Exception as error:
            logger.error(f"‚ùå Worker {self.worker_id}: Erreur scraping CPC pour {domain}: {error}")
            return '0'
'''
            
            # Trouver l'endroit pour ins√©rer la m√©thode (apr√®s la derni√®re m√©thode de scraping)
            insertion_point = content.find('async def scrape_conversion_rate_via_api(self, domain: str) -> str:')
            if insertion_point == -1:
                # Si pas trouv√©, chercher un autre point d'insertion
                insertion_point = content.find('class ParallelProductionScraper:')
                if insertion_point != -1:
                    # Ins√©rer apr√®s la classe
                    class_end = content.find('\n\n', insertion_point)
                    if class_end != -1:
                        insertion_point = class_end + 2
                    else:
                        insertion_point = len(content)
                else:
                    insertion_point = len(content)
            
            # Ins√©rer la m√©thode
            new_content = content[:insertion_point] + cpc_method + content[insertion_point:]
            
            # √âcrire le fichier modifi√©
            with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            logger.info("‚úÖ M√©thode CPC int√©gr√©e avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur int√©gration m√©thode CPC: {e}")
            return False
    
    def _step4_integrate_workflow(self) -> bool:
        """√âTAPE 4: Int√©gration dans le workflow principal"""
        logger.info("üîÑ √âTAPE 4: Int√©gration dans le workflow principal")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Trouver la m√©thode scrape_domain_overview et ajouter l'appel CPC
            scrape_method_start = content.find('async def scrape_domain_overview(self, domain: str, date_range: str, existing_metrics: Dict[str, str] = None):')
            
            if scrape_method_start == -1:
                logger.error("‚ùå M√©thode scrape_domain_overview non trouv√©e")
                return False
            
            # Trouver la fin de la m√©thode (rechercher la m√©thode suivante)
            next_method_start = content.find('\n    async def ', scrape_method_start + 1)
            if next_method_start == -1:
                next_method_start = len(content)
            
            # Extraire la m√©thode actuelle
            method_content = content[scrape_method_start:next_method_start]
            
            # Ajouter l'appel CPC avant le return
            cpc_integration = '''
            # CPC - Cost Per Click
            if 'cpc' not in existing_metrics or existing_metrics.get('cpc') in ['', '0', None]:
                cpc_value = await self.scrape_cpc_via_api(domain)
                metrics['cpc'] = cpc_value
                logger.info(f"üí∞ Worker {self.worker_id}: CPC extrait: {cpc_value}")
            else:
                metrics['cpc'] = existing_metrics.get('cpc', '0')
                logger.info(f"üí∞ Worker {self.worker_id}: CPC existant: {existing_metrics.get('cpc')}")
'''
            
            # Ins√©rer avant le return
            return_pos = method_content.rfind('return metrics')
            if return_pos != -1:
                new_method_content = method_content[:return_pos] + cpc_integration + '\n        ' + method_content[return_pos:]
            else:
                # Si pas de return trouv√©, ajouter √† la fin
                new_method_content = method_content + cpc_integration
            
            # Reconstituer le fichier
            new_content = content[:scrape_method_start] + new_method_content + content[next_method_start:]
            
            # √âcrire le fichier modifi√©
            with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            logger.info("‚úÖ CPC int√©gr√© dans le workflow principal")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur int√©gration workflow: {e}")
            return False
    
    def _step5_update_counters(self) -> bool:
        """√âTAPE 5: Mise √† jour des compteurs"""
        logger.info("üìä √âTAPE 5: Mise √† jour des compteurs")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ajouter CPC aux compteurs de m√©triques
            counter_update = ''',
            'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}'''
            
            # Trouver la section des compteurs
            counters_start = content.find("self.metrics_count = {")
            if counters_start != -1:
                # Trouver la fin de la section
                counters_end = content.find('}', counters_start)
                if counters_end != -1:
                    # Ins√©rer avant la derni√®re accolade
                    new_content = content[:counters_end] + counter_update + content[counters_end:]
                    
                    # √âcrire le fichier modifi√©
                    with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    
                    logger.info("‚úÖ Compteurs CPC mis √† jour")
                    return True
            
            logger.warning("‚ö†Ô∏è Section compteurs non trouv√©e, continuer sans mise √† jour")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour compteurs: {e}")
            return False
    
    def _step6_validation(self) -> bool:
        """√âTAPE 6: Test de validation"""
        logger.info("üß™ √âTAPE 6: Test de validation")
        
        try:
            # Test de syntaxe Python
            result = subprocess.run([
                'python3', '-m', 'py_compile', str(self.production_scraper_file)
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"‚ùå Erreur de syntaxe Python: {result.stderr}")
                return False
            
            logger.info("‚úÖ Validation syntaxe Python r√©ussie")
            
            # V√©rifier que les m√©thodes CPC sont pr√©sentes
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if 'async def scrape_cpc_via_api' not in content:
                logger.error("‚ùå M√©thode scrape_cpc_via_api non trouv√©e")
                return False
            
            if 'scrape_cpc_via_api' not in content:
                logger.error("‚ùå Appel √† scrape_cpc_via_api non trouv√©")
                return False
            
            logger.info("‚úÖ Validation des m√©thodes CPC r√©ussie")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation: {e}")
            return False
    
    def _rollback(self):
        """Rollback en cas d'erreur"""
        logger.info("üîÑ Rollback en cours...")
        
        try:
            # Restaurer depuis le backup le plus r√©cent
            if self.backup_dir.exists():
                backup_files = list(self.backup_dir.glob("production_scraper_parallel_backup_*.py"))
                if backup_files:
                    latest_backup = max(backup_files, key=os.path.getctime)
                    shutil.copy2(latest_backup, self.production_scraper_file)
                    logger.info(f"‚úÖ Rollback r√©ussi depuis: {latest_backup}")
                    return
            
            logger.warning("‚ö†Ô∏è Aucun backup trouv√© pour le rollback")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du rollback: {e}")

def main():
    """Fonction principale"""
    logger.info("üéØ INT√âGRATION DE LA M√âTRIQUE CPC DANS LE SCRAPER SEM")
    logger.info("=" * 60)
    
    integrator = CPCIntegration()
    success = integrator.integrate_cpc_metric()
    
    if success:
        logger.info("üéâ INT√âGRATION CPC TERMIN√âE AVEC SUCC√àS!")
        logger.info("üìã Prochaines √©tapes:")
        logger.info("   1. Tester le scraper avec: python3 menu_workers.py")
        logger.info("   2. Choisir l'option 1 (Lancer les workers SEM)")
        logger.info("   3. V√©rifier que CPC est extrait correctement")
        logger.info("   4. Valider les donn√©es en base de donn√©es")
    else:
        logger.error("‚ùå INT√âGRATION CPC √âCHOU√âE")
        logger.info("üîÑ Rollback effectu√© automatiquement")
    
    return success

if __name__ == "__main__":
    main()
