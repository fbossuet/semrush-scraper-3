#!/usr/bin/env python3
"""
IntÃ©gration de la mÃ©trique CPC dans le scraper SEM
Script d'intÃ©gration automatisÃ©e pour ajouter CPC au scraper MyToolsPlan
"""

import os
import sys
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CPCIntegration:
    """IntÃ©gration de la mÃ©trique CPC dans le scraper SEM"""
    
    def __init__(self):
        self.sem_scraper_path = Path("/home/ubuntu/sem-scraper-final")
        self.trendtrack_db_path = Path("/home/ubuntu/trendtrack-scraper-final/data/trendtrack.db")
        self.production_scraper_file = self.sem_scraper_path / "production_scraper_parallel.py"
        self.backup_dir = self.sem_scraper_path / "backup_cpc_integration"
        
    def integrate_cpc_metric(self):
        """IntÃ¨gre la mÃ©trique CPC dans le scraper SEM"""
        try:
            logger.info("ğŸš€ DÃ©but de l'intÃ©gration de la mÃ©trique CPC")
            
            # Ã‰TAPE 1: VÃ©rifications prÃ©liminaires
            if not self._step1_verifications():
                return False
            
            # Ã‰TAPE 2: Backup du scraper existant
            if not self._step2_backup():
                return False
            
            # Ã‰TAPE 3: IntÃ©gration de la mÃ©thode CPC
            if not self._step3_integrate_cpc_method():
                return False
            
            # Ã‰TAPE 4: IntÃ©gration dans le workflow principal
            if not self._step4_integrate_workflow():
                return False
            
            # Ã‰TAPE 5: Mise Ã  jour des compteurs
            if not self._step5_update_counters():
                return False
            
            # Ã‰TAPE 6: Test de validation
            if not self._step6_validation():
                return False
            
            logger.info("âœ… IntÃ©gration de la mÃ©trique CPC terminÃ©e avec succÃ¨s!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'intÃ©gration CPC: {e}")
            self._rollback()
            return False
    
    def _step1_verifications(self) -> bool:
        """Ã‰TAPE 1: VÃ©rifications prÃ©liminaires"""
        logger.info("ğŸ” Ã‰TAPE 1: VÃ©rifications prÃ©liminaires")
        
        # VÃ©rifier que le fichier scraper existe
        if not self.production_scraper_file.exists():
            logger.error(f"âŒ Fichier scraper non trouvÃ©: {self.production_scraper_file}")
            return False
        
        # VÃ©rifier que la base de donnÃ©es existe
        if not self.trendtrack_db_path.exists():
            logger.error(f"âŒ Base de donnÃ©es non trouvÃ©e: {self.trendtrack_db_path}")
            return False
        
        # VÃ©rifier que la colonne CPC existe en base
        import sqlite3
        try:
            conn = sqlite3.connect(self.trendtrack_db_path)
            cursor = conn.cursor()
            cursor.execute("PRAGMA table_info(analytics)")
            columns = [row[1] for row in cursor.fetchall()]
            conn.close()
            
            if 'cpc' not in columns:
                logger.error("âŒ Colonne 'cpc' non trouvÃ©e dans la table analytics")
                return False
            
            logger.info("âœ… Colonne 'cpc' vÃ©rifiÃ©e en base de donnÃ©es")
            
        except Exception as e:
            logger.error(f"âŒ Erreur vÃ©rification base de donnÃ©es: {e}")
            return False
        
        logger.info("âœ… VÃ©rifications prÃ©liminaires rÃ©ussies")
        return True
    
    def _step2_backup(self) -> bool:
        """Ã‰TAPE 2: Backup du scraper existant"""
        logger.info("ğŸ’¾ Ã‰TAPE 2: Backup du scraper existant")
        
        try:
            # CrÃ©er le rÃ©pertoire de backup
            self.backup_dir.mkdir(exist_ok=True)
            
            # Backup du fichier principal
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"production_scraper_parallel_backup_{timestamp}.py"
            shutil.copy2(self.production_scraper_file, backup_file)
            
            logger.info(f"âœ… Backup crÃ©Ã©: {backup_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du backup: {e}")
            return False
    
    def _step3_integrate_cpc_method(self) -> bool:
        """Ã‰TAPE 3: IntÃ©gration de la mÃ©thode CPC"""
        logger.info("ğŸ”§ Ã‰TAPE 3: IntÃ©gration de la mÃ©thode CPC")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # MÃ©thode CPC Ã  ajouter
            cpc_method = '''
    async def scrape_cpc_via_api(self, domain: str) -> str:
        """
        RÃ©cupÃ¨re le CPC (Cost Per Click) via l'API MyToolsPlan
        Utilise l'API pour extraire les donnÃ©es de coÃ»t par clic
        """
        try:
            logger.info(f"ğŸ’° Worker {self.worker_id}: Scraping CPC pour {domain}")
            
            # Throttling avant appel API
            await stealth_system.throttle_api_call(self.worker_id, "cpc")
            
            # Nettoyer le domaine
            clean_domain = domain.replace('https://', '').replace('http://', '').replace('www.', '').strip('/')
            
            # Navigation vers sam.mytoolsplan.xyz pour les appels API
            await self.page.goto("https://sam.mytoolsplan.xyz/analytics/overview/", wait_until='domcontentloaded', timeout=30000)
            await stealth_system.human_pause(self.worker_id, "session")
            
            # Appel API pour rÃ©cupÃ©rer le CPC
            # Note: Ã€ adapter selon l'endpoint API MyToolsPlan pour CPC
            stealth_headers = stealth_system.get_stealth_headers()
            
            # Exemple d'appel API (Ã  adapter selon l'API rÃ©elle)
            fetch_code = f"""
                async () => {{
                    try {{
                        const response = await fetch('/dpa/rpc', {{
                            method: 'POST',
                            headers: {{
                                'Content-Type': 'application/json',
                                'User-Agent': '{stealth_headers.get('User-Agent', '')}',
                                'Accept-Language': '{stealth_headers.get('Accept-Language', '')}',
                                'Accept': '{stealth_headers.get('Accept', '')}'
                            }},
                            body: JSON.stringify({{
                                "jsonrpc": "2.0",
                                "id": 1,
                                "method": "cpc.getData",
                                "params": {{
                                    "domain": "{clean_domain}",
                                    "date": "{self.target_date}"
                                }}
                            }})
                        }});
                        
                        if (!response.ok) {{
                            throw new Error(`HTTP ${{response.status}}: ${{response.statusText}}`);
                        }}
                        
                        const data = await response.json();
                        return data;
                        
                    }} catch (error) {{
                        console.error('Erreur API CPC:', error);
                        return null;
                    }}
                }}
            """
            
            # ExÃ©cuter l'appel API
            result = await self.page.evaluate(fetch_code)
            
            if result and result.get('result'):
                cpc_data = result['result']
                # Extraire la valeur CPC (Ã  adapter selon la structure de rÃ©ponse)
                cpc_value = cpc_data.get('cpc', cpc_data.get('cost_per_click', '0'))
                
                # Convertir en format numÃ©rique si nÃ©cessaire
                try:
                    cpc_numeric = float(str(cpc_value).replace('$', '').replace(',', ''))
                    logger.info(f"âœ… Worker {self.worker_id}: CPC rÃ©cupÃ©rÃ©: ${cpc_numeric:.2f}")
                    return str(cpc_numeric)
                except (ValueError, TypeError):
                    logger.warning(f"âš ï¸ Worker {self.worker_id}: CPC non numÃ©rique: {cpc_value}")
                    return '0'
            else:
                logger.warning(f"âš ï¸ Worker {self.worker_id}: Pas de donnÃ©es CPC pour {domain}")
                return '0'
                
        except Exception as error:
            logger.error(f"âŒ Worker {self.worker_id}: Erreur scraping CPC pour {domain}: {error}")
            return '0'
'''
            
            # Trouver l'endroit pour insÃ©rer la mÃ©thode (aprÃ¨s la derniÃ¨re mÃ©thode de scraping)
            insertion_point = content.find('async def scrape_conversion_rate_via_api(self, domain: str) -> str:')
            if insertion_point == -1:
                # Si pas trouvÃ©, chercher un autre point d'insertion
                insertion_point = content.find('class ParallelProductionScraper:')
                if insertion_point != -1:
                    # InsÃ©rer aprÃ¨s la classe
                    class_end = content.find('\n\n', insertion_point)
                    if class_end != -1:
                        insertion_point = class_end + 2
                    else:
                        insertion_point = len(content)
                else:
                    insertion_point = len(content)
            
            # InsÃ©rer la mÃ©thode
            new_content = content[:insertion_point] + cpc_method + content[insertion_point:]
            
            # Ã‰crire le fichier modifiÃ©
            with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            logger.info("âœ… MÃ©thode CPC intÃ©grÃ©e avec succÃ¨s")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur intÃ©gration mÃ©thode CPC: {e}")
            return False
    
    def _step4_integrate_workflow(self) -> bool:
        """Ã‰TAPE 4: IntÃ©gration dans le workflow principal"""
        logger.info("ğŸ”„ Ã‰TAPE 4: IntÃ©gration dans le workflow principal")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Trouver la mÃ©thode scrape_domain_overview et ajouter l'appel CPC
            scrape_method_start = content.find('async def scrape_domain_overview(self, domain: str, date_range: str, existing_metrics: Dict[str, str] = None):')
            
            if scrape_method_start == -1:
                logger.error("âŒ MÃ©thode scrape_domain_overview non trouvÃ©e")
                return False
            
            # Trouver la fin de la mÃ©thode (rechercher la mÃ©thode suivante)
            next_method_start = content.find('\n    async def ', scrape_method_start + 1)
            if next_method_start == -1:
                next_method_start = len(content)
            
            # Extraire la mÃ©thode actuelle
            method_content = content[scrape_method_start:next_method_start]
            
            # Ajouter l'appel CPC avant le return
            cpc_integration = '''
            # CPC - Cost Per Click
            if 'cpc' not in existing_metrics or existing_metrics.get('cpc') in ['', '0', None]:
                cpc_value = await self.scrape_cpc_via_api(domain)
                metrics['cpc'] = cpc_value
                logger.info(f"ğŸ’° Worker {self.worker_id}: CPC extrait: {cpc_value}")
            else:
                metrics['cpc'] = existing_metrics.get('cpc', '0')
                logger.info(f"ğŸ’° Worker {self.worker_id}: CPC existant: {existing_metrics.get('cpc')}")
'''
            
            # InsÃ©rer avant le return
            return_pos = method_content.rfind('return metrics')
            if return_pos != -1:
                new_method_content = method_content[:return_pos] + cpc_integration + '\n        ' + method_content[return_pos:]
            else:
                # Si pas de return trouvÃ©, ajouter Ã  la fin
                new_method_content = method_content + cpc_integration
            
            # Reconstituer le fichier
            new_content = content[:scrape_method_start] + new_method_content + content[next_method_start:]
            
            # Ã‰crire le fichier modifiÃ©
            with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            logger.info("âœ… CPC intÃ©grÃ© dans le workflow principal")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur intÃ©gration workflow: {e}")
            return False
    
    def _step5_update_counters(self) -> bool:
        """Ã‰TAPE 5: Mise Ã  jour des compteurs"""
        logger.info("ğŸ“Š Ã‰TAPE 5: Mise Ã  jour des compteurs")
        
        try:
            # Lire le fichier actuel
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ajouter CPC aux compteurs de mÃ©triques
            counter_update = ''',
            'cpc': {'found': 0, 'not_found': 0, 'skipped': 0}'''
            
            # Trouver la section des compteurs
            counters_start = content.find("self.metrics_count = {")
            if counters_start != -1:
                # Trouver la fin de la section
                counters_end = content.find('}', counters_start)
                if counters_end != -1:
                    # InsÃ©rer avant la derniÃ¨re accolade
                    new_content = content[:counters_end] + counter_update + content[counters_end:]
                    
                    # Ã‰crire le fichier modifiÃ©
                    with open(self.production_scraper_file, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    
                    logger.info("âœ… Compteurs CPC mis Ã  jour")
                    return True
            
            logger.warning("âš ï¸ Section compteurs non trouvÃ©e, continuer sans mise Ã  jour")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur mise Ã  jour compteurs: {e}")
            return False
    
    def _step6_validation(self) -> bool:
        """Ã‰TAPE 6: Test de validation"""
        logger.info("ğŸ§ª Ã‰TAPE 6: Test de validation")
        
        try:
            # Test de syntaxe Python
            result = subprocess.run([
                'python3', '-m', 'py_compile', str(self.production_scraper_file)
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"âŒ Erreur de syntaxe Python: {result.stderr}")
                return False
            
            logger.info("âœ… Validation syntaxe Python rÃ©ussie")
            
            # VÃ©rifier que les mÃ©thodes CPC sont prÃ©sentes
            with open(self.production_scraper_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if 'async def scrape_cpc_via_api' not in content:
                logger.error("âŒ MÃ©thode scrape_cpc_via_api non trouvÃ©e")
                return False
            
            if 'scrape_cpc_via_api' not in content:
                logger.error("âŒ Appel Ã  scrape_cpc_via_api non trouvÃ©")
                return False
            
            logger.info("âœ… Validation des mÃ©thodes CPC rÃ©ussie")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur validation: {e}")
            return False
    
    def _rollback(self):
        """Rollback en cas d'erreur"""
        logger.info("ğŸ”„ Rollback en cours...")
        
        try:
            # Restaurer depuis le backup le plus rÃ©cent
            if self.backup_dir.exists():
                backup_files = list(self.backup_dir.glob("production_scraper_parallel_backup_*.py"))
                if backup_files:
                    latest_backup = max(backup_files, key=os.path.getctime)
                    shutil.copy2(latest_backup, self.production_scraper_file)
                    logger.info(f"âœ… Rollback rÃ©ussi depuis: {latest_backup}")
                    return
            
            logger.warning("âš ï¸ Aucun backup trouvÃ© pour le rollback")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du rollback: {e}")

def main():
    """Fonction principale"""
    logger.info("ğŸ¯ INTÃ‰GRATION DE LA MÃ‰TRIQUE CPC DANS LE SCRAPER SEM")
    logger.info("=" * 60)
    
    integrator = CPCIntegration()
    success = integrator.integrate_cpc_metric()
    
    if success:
        logger.info("ğŸ‰ INTÃ‰GRATION CPC TERMINÃ‰E AVEC SUCCÃˆS!")
        logger.info("ğŸ“‹ Prochaines Ã©tapes:")
        logger.info("   1. Tester le scraper avec: python3 menu_workers.py")
        logger.info("   2. Choisir l'option 1 (Lancer les workers SEM)")
        logger.info("   3. VÃ©rifier que CPC est extrait correctement")
        logger.info("   4. Valider les donnÃ©es en base de donnÃ©es")
    else:
        logger.error("âŒ INTÃ‰GRATION CPC Ã‰CHOUÃ‰E")
        logger.info("ğŸ”„ Rollback effectuÃ© automatiquement")
    
    return success

if __name__ == "__main__":
    main()
