#!/usr/bin/env python3
"""
Version modifi√©e du scraper principal pour mettre √† jour uniquement conversion_rate
Ne fait que Traffic Analysis (pas Domain Overview ni Organic Search)
"""

import asyncio
import sys
import os
from pathlib import Path
from datetime import datetime, timezone, timedelta
import logging

# Ajouter le r√©pertoire parent au path
sys.path.append(str(Path(__file__).parent))

from production_scraper import ProductionScraper
import trendtrack_api as api

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/conversion_rate_only_{datetime.now(timezone.utc).isoformat()}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConversionRateOnlyScraper(ProductionScraper):
    """Scraper sp√©cialis√© pour conversion_rate uniquement"""
    
    async def scrape_conversion_rate_only(self, domain: str):
        """Scrape uniquement Traffic Analysis pour r√©cup√©rer conversion_rate"""
        try:
            logger.info(f"üîÑ Scraping conversion_rate pour {domain}")
            
            # Scraping Traffic Analysis uniquement
            logger.info("üìä √âTAPE: Scraping Traffic Analysis...")
            traffic_success = await self.scrape_traffic_analysis(domain)
            
            if traffic_success:
                # R√©cup√©rer les donn√©es de session
                traffic_data = self.session_data.get('data', {}).get('traffic_analysis', {})
                conversion_rate = traffic_data.get('purchase_conversion', '')
                
                if conversion_rate and conversion_rate not in ["S√©lecteur non trouv√©", "Erreur"]:
                    # Trouver le shop_id
                    all_shops = api.get_all_shops()
                    shop_id = None
                    for shop in all_shops:
                        if shop.get('shop_url') == domain:
                            shop_id = shop.get('id')
                            break
                    
                    if shop_id:
                        # Mise √† jour directe en SQL
                        conn = api._get_connection()
                        cursor = conn.cursor()
                        cursor.execute("""
                            UPDATE analytics 
                            SET conversion_rate = ?, updated_at = ? 
                            WHERE shop_id = ?
                        """, (conversion_rate, datetime.now(timezone.utc), shop_id))
                        conn.commit()
                        logger.info(f"‚úÖ Conversion rate mis √† jour pour {domain}: {conversion_rate}")
                        return True
                    else:
                        logger.error(f"‚ùå Shop ID non trouv√© pour {domain}")
                        return False
                else:
                    logger.warning(f"‚ö†Ô∏è Conversion rate non trouv√© pour {domain}")
                    return False
            else:
                logger.error(f"‚ùå √âchec scraping Traffic Analysis pour {domain}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping conversion_rate pour {domain}: {e}")
            return False

async def main():
    """Fonction principale"""
    logger.info("üöÄ D√âBUT MISE √Ä JOUR CONVERSION RATE ONLY")
    
    try:
        # R√©cup√©rer tous les shops
        all_shops = api.get_all_shops()
        logger.info(f"üìä {len(all_shops)} shops trouv√©s")
        
        # Initialiser le scraper
        scraper = ConversionRateOnlyScraper()
        await scraper.setup_browser()
        
        # Authentification MyToolsPlan
        auth_success = await scraper.authenticate_mytoolsplan()
        if not auth_success:
            logger.error("‚ùå √âchec de l'authentification - Arr√™t du scraping")
            return
        
        success_count = 0
        error_count = 0
        
        for shop in all_shops:
            shop_url = shop.get('shop_url')
            
            if not shop_url:
                continue
                
            try:
                success = await scraper.scrape_conversion_rate_only(shop_url)
                if success:
                    success_count += 1
                else:
                    error_count += 1
                    
                # Pause entre les shops
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur traitement {shop_url}: {e}")
                error_count += 1
        
        logger.info(f"‚úÖ MISE √Ä JOUR TERMIN√âE: {success_count} r√©ussis, {error_count} √©checs")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur critique: {e}")
    finally:
        if 'scraper' in locals():
            await scraper.browser.close()

if __name__ == "__main__":
    asyncio.run(main())
