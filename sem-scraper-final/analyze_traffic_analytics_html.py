#!/usr/bin/env python3
"""
Analyse de la structure HTML de Traffic Analytics
Pour comprendre o√π sont stock√©es les m√©triques Visits et Purchase Conversion
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from production_scraper_current import ProductionScraper
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('analyze_traffic_analytics_html.log')
    ]
)
logger = logging.getLogger(__name__)

async def analyze_traffic_analytics_html():
    """Analyse compl√®te de la structure HTML Traffic Analytics"""
    
    scraper = None
    
    try:
        logger.info("üîç ANALYSE STRUCTURE HTML TRAFFIC ANALYTICS")
        logger.info("=" * 60)
        
        # Initialiser le scraper
        scraper = ProductionScraper()
        await scraper.setup_browser()
        
        # Authentification
        logger.info("üîê Authentification...")
        await scraper.check_and_authenticate()
        
        # Test avec spanx.com
        domain = "https://spanx.com"
        domain_clean = domain.replace("https://", "").replace("http://", "")
        
        logger.info(f"üìä Analyse pour: {domain}")
        
        # √âtape 1: Cr√©er le dossier Traffic Analytics
        logger.info("\nüìÅ √âTAPE 1: Cr√©ation du dossier Traffic Analytics")
        folder_created = await scraper.create_traffic_folder(domain)
        
        if not folder_created:
            logger.error("‚ùå Impossible de cr√©er le dossier Traffic Analytics")
            return
        
        logger.info("‚úÖ Dossier Traffic Analytics cr√©√©/s√©lectionn√©")
        
        # √âtape 2: Analyser la structure HTML de la page
        logger.info("\nüìä √âTAPE 2: Analyse de la structure HTML")
        
        current_url = scraper.page.url
        page_title = await scraper.page.title()
        logger.info(f"üìç URL actuelle: {current_url}")
        logger.info(f"üìÑ Titre de la page: {page_title}")
        
        # Attendre que la page se charge compl√®tement
        await asyncio.sleep(5)
        
        # Analyser les √©l√©ments avec data-testid
        logger.info("\nüîç Analyse des √©l√©ments data-testid:")
        testid_elements = await scraper.page.query_selector_all('[data-testid]')
        logger.info(f"üìä {len(testid_elements)} √©l√©ments data-testid trouv√©s")
        
        testid_values = set()
        for element in testid_elements:
            try:
                testid = await element.get_attribute('data-testid')
                if testid:
                    testid_values.add(testid)
            except:
                pass
        
        logger.info(f"üìã Valeurs data-testid uniques: {sorted(testid_values)}")
        
        # Analyser sp√©cifiquement les m√©triques Visits et Conversion
        logger.info("\nüîç Analyse sp√©cifique des m√©triques:")
        
        # Chercher les √©l√©ments contenant "visits"
        visits_elements = await scraper.page.query_selector_all('[data-testid*="visit"], [data-testid*="visits"], [class*="visit"], [class*="visits"]')
        logger.info(f"üìä {len(visits_elements)} √©l√©ments li√©s aux visits trouv√©s")
        
        for i, element in enumerate(visits_elements[:5]):  # Limiter √† 5
            try:
                testid = await element.get_attribute('data-testid')
                class_name = await element.get_attribute('class')
                text_content = await element.inner_text()
                logger.info(f"   {i+1}. testid='{testid}' class='{class_name}' text='{text_content[:50]}...'")
            except:
                pass
        
        # Chercher les √©l√©ments contenant "conversion"
        conversion_elements = await scraper.page.query_selector_all('[data-testid*="conversion"], [class*="conversion"]')
        logger.info(f"üìä {len(conversion_elements)} √©l√©ments li√©s √† la conversion trouv√©s")
        
        for i, element in enumerate(conversion_elements[:5]):  # Limiter √† 5
            try:
                testid = await element.get_attribute('data-testid')
                class_name = await element.get_attribute('class')
                text_content = await element.inner_text()
                logger.info(f"   {i+1}. testid='{testid}' class='{class_name}' text='{text_content[:50]}...'")
            except:
                pass
        
        # Analyser les tableaux HTML
        logger.info("\nüîç Analyse des tableaux HTML:")
        tables = await scraper.page.query_selector_all('table, [role="table"], [role="grid"]')
        logger.info(f"üìä {len(tables)} tableaux trouv√©s")
        
        for i, table in enumerate(tables[:3]):  # Limiter √† 3
            try:
                rows = await table.query_selector_all('tr, [role="row"]')
                logger.info(f"   Tableau {i+1}: {len(rows)} lignes")
                
                # Analyser les premi√®res lignes
                for j, row in enumerate(rows[:3]):
                    cells = await row.query_selector_all('td, th, [role="cell"], [role="gridcell"]')
                    cell_texts = []
                    for cell in cells:
                        try:
                            text = await cell.inner_text()
                            cell_texts.append(text.strip())
                        except:
                            pass
                    logger.info(f"     Ligne {j+1}: {cell_texts}")
            except:
                pass
        
        # Analyser les cartes de r√©sum√© (summary cards)
        logger.info("\nüîç Analyse des cartes de r√©sum√©:")
        summary_cards = await scraper.page.query_selector_all('[class*="summary"], [class*="card"], [class*="metric"]')
        logger.info(f"üìä {len(summary_cards)} cartes de r√©sum√© trouv√©es")
        
        for i, card in enumerate(summary_cards[:10]):  # Limiter √† 10
            try:
                class_name = await card.get_attribute('class')
                text_content = await element.inner_text()
                if any(keyword in text_content.lower() for keyword in ['visit', 'conversion', 'traffic', 'metric']):
                    logger.info(f"   {i+1}. class='{class_name}' text='{text_content[:100]}...'")
            except:
                pass
        
        # Analyser le contenu JavaScript/React
        logger.info("\nüîç Analyse du contenu JavaScript/React:")
        
        # Chercher les donn√©es dans window ou des variables globales
        js_data = await scraper.page.evaluate("""
            () => {
                const result = {
                    windowData: {},
                    reactData: {},
                    metrics: []
                };
                
                // Chercher dans window
                if (window.__INITIAL_STATE__) {
                    result.windowData.initialState = window.__INITIAL_STATE__;
                }
                if (window.__NEXT_DATA__) {
                    result.windowData.nextData = window.__NEXT_DATA__;
                }
                
                // Chercher des √©l√©ments avec des donn√©es React
                const elements = document.querySelectorAll('[data-reactroot], [data-reactid]');
                result.reactData.elementsCount = elements.length;
                
                // Chercher des m√©triques dans le texte
                const allText = document.body.innerText;
                const metrics = [];
                
                // Patterns pour les m√©triques
                const patterns = [
                    /(\d+(?:\.\d+)?[KMB]?)\s*(?:visits?|traffic)/gi,
                    /(\d+(?:\.\d+)?%?)\s*(?:conversion|rate)/gi,
                    /(\d+(?:\.\d+)?[KMB]?)\s*(?:organic|paid)/gi
                ];
                
                patterns.forEach(pattern => {
                    const matches = allText.match(pattern);
                    if (matches) {
                        metrics.push(...matches);
                    }
                });
                
                result.metrics = [...new Set(metrics)];
                
                return result;
            }
        """)
        
        logger.info(f"üìã Donn√©es JavaScript trouv√©es:")
        logger.info(f"   ‚Ä¢ √âl√©ments React: {js_data.get('reactData', {}).get('elementsCount', 0)}")
        logger.info(f"   ‚Ä¢ M√©triques d√©tect√©es: {js_data.get('metrics', [])}")
        
        # Sauvegarder le HTML complet pour analyse
        logger.info("\nüíæ Sauvegarde du HTML pour analyse...")
        html_content = await scraper.page.content()
        
        with open(f'traffic_analytics_html_{domain_clean.replace(".", "_")}.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"‚úÖ HTML sauvegard√© dans: traffic_analytics_html_{domain_clean.replace('.', '_')}.html")
        
        # R√©sum√© de l'analyse
        logger.info("\nüéØ R√âSUM√â DE L'ANALYSE")
        logger.info("=" * 60)
        logger.info(f"üìä √âl√©ments data-testid: {len(testid_elements)}")
        logger.info(f"üìä Tableaux HTML: {len(tables)}")
        logger.info(f"üìä Cartes de r√©sum√©: {len(summary_cards)}")
        logger.info(f"üìä M√©triques JavaScript: {len(js_data.get('metrics', []))}")
        
        # Recommandations
        logger.info("\nüí° RECOMMANDATIONS:")
        if len(testid_elements) > 0:
            logger.info("‚úÖ Utiliser les s√©lecteurs data-testid pour le scraping")
        if len(tables) > 0:
            logger.info("‚úÖ Analyser les tableaux HTML pour les m√©triques")
        if len(js_data.get('metrics', [])) > 0:
            logger.info("‚úÖ Les m√©triques sont pr√©sentes dans le contenu JavaScript")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur critique de l'analyse: {e}")
        
    finally:
        # Nettoyage
        if scraper and scraper.browser:
            logger.info("üßπ Nettoyage...")
            await scraper.browser.close()
        
        logger.info("‚úÖ Analyse termin√©e")

if __name__ == "__main__":
    asyncio.run(analyze_traffic_analytics_html())
