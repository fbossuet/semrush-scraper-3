#!/usr/bin/env python3
"""
API REST pour les donn√©es TrendTrack
Permet de r√©cup√©rer les donn√©es avec des filtres via HTTP
"""

from fastapi import FastAPI, Query, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime, timezone
import logging
import subprocess
import asyncio

# Import de notre API
from trendtrack_api import TrendTrackAPI, get_database_path

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cr√©er l'application FastAPI
app = FastAPI(
    title="TrendTrack API",
    description="""
    API pour r√©cup√©rer et mettre √† jour les donn√©es de scraping TrendTrack
    
    ## Endpoints GET (Lecture)
    - `/shops` - R√©cup√©rer toutes les boutiques
    - `/shops/with-analytics` - R√©cup√©rer les boutiques avec leurs m√©triques
    - `/shops/filter` - R√©cup√©rer les boutiques avec filtres
    - `/shops/{shop_id}` - R√©cup√©rer une boutique par ID
    - `/analytics/{shop_id}` - R√©cup√©rer les analytics d'une boutique
    - `/stats` - Statistiques g√©n√©rales
    
    ## Endpoints POST (√âcriture - SEM-Scraper)
    - `/update-shop-analytics` - Met √† jour les analytics d'une boutique
    - `/mark-shop-failed` - Marque une boutique comme √©chou√©e
    - `/record-selector-performance` - Enregistre les performances d'un s√©lecteur
    - `/get-selector-performances` - R√©cup√®re les performances r√©centes d'un s√©lecteur
    - `/calculate-adaptive-timeout` - Calcule un timeout adaptatif
    """,
    version="1.0.0"
)

# Ajouter CORS pour permettre les requ√™tes depuis n'importe o√π
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic pour les endpoints POST
class AnalyticsUpdateRequest(BaseModel):
    """
    Mod√®le pour la mise √† jour des analytics d'une boutique
    """
    shop_id: int = Field(..., description="ID de la boutique √† mettre √† jour")
    analytics_data: Dict[str, Any] = Field(..., description="Donn√©es analytics √† sauvegarder", example={
        "organic_traffic": "1,234",
        "bounce_rate": "45.2%",
        "average_visit_duration": "2m 30s",
        "branded_traffic": "567",
        "conversion_rate": "3.2%",
        "visits": "5,678",
        "scraping_status": "completed"
    })

class ShopFailureRequest(BaseModel):
    """
    Mod√®le pour marquer une boutique comme √©chou√©e
    """
    shop_id: int = Field(..., description="ID de la boutique √† marquer comme √©chou√©e")
    error_message: str = Field(..., description="Message d'erreur d√©crivant l'√©chec", example="Timeout lors du scraping des m√©triques Traffic Analysis")

class SelectorPerformanceRequest(BaseModel):
    """
    Mod√®le pour enregistrer les performances d'un s√©lecteur
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    success: bool = Field(..., description="Si le s√©lecteur a √©t√© trouv√© avec succ√®s")
    response_time_ms: int = Field(..., description="Temps de r√©ponse en millisecondes", ge=0)
    page_load_time_ms: Optional[int] = Field(None, description="Temps de chargement de la page en ms", ge=0)

class SelectorPerformanceQuery(BaseModel):
    """
    Mod√®le pour r√©cup√©rer les performances d'un s√©lecteur
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    limit: Optional[int] = Field(20, description="Nombre maximum de performances √† r√©cup√©rer", ge=1, le=100)

class AdaptiveTimeoutRequest(BaseModel):
    """
    Mod√®le pour calculer un timeout adaptatif
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    base_timeout: Optional[int] = Field(30000, description="Timeout de base en millisecondes", ge=1000, le=300000)

# Initialiser l'API TrendTrack
api = TrendTrackAPI(get_database_path())

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "TrendTrack API",
        "version": "1.0.0",
        "endpoints": {
            "shops": "/shops - R√©cup√©rer toutes les boutiques",
            "shops_filtered": "/shops/filter - R√©cup√©rer les boutiques avec filtres (inclut include_analytics)",
            "shop_by_id": "/shops/{shop_id} - R√©cup√©rer une boutique par ID",
            "analytics": "/analytics/{shop_id} - R√©cup√©rer les analytics d'une boutique",
            "stats": "/stats - Statistiques g√©n√©rales"
        }
    }

@app.get("/shops")
async def get_all_shops():
    """R√©cup√©rer toutes les boutiques"""
    try:
        shops = api.get_all_shops()
        return {
            "success": True,
            "count": len(shops),
            "data": shops
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/with-analytics")
async def get_all_shops_with_analytics():
    """R√©cup√©rer toutes les boutiques avec leurs m√©triques analytics"""
    try:
        all_shops = api.get_all_shops()
        shops_with_analytics = []
        
        for shop in all_shops:
            shop_with_analytics = shop.copy()
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_with_analytics.update(analytics)
            shops_with_analytics.append(shop_with_analytics)
        
        return {
            "success": True,
            "count": len(shops_with_analytics),
            "data": shops_with_analytics
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutiques avec analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/filter")
async def get_filtered_shops(
    status: Optional[str] = Query(None, description="Filtrer par status (completed, na, partial, failed)"),
    date_from: Optional[str] = Query(None, description="Date de d√©but (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="Date de fin (YYYY-MM-DD)"),
    limit: Optional[int] = Query(None, description="Nombre maximum de r√©sultats"),
    domain: Optional[str] = Query(None, description="Filtrer par domaine"),
    include_analytics: Optional[bool] = Query(True, description="Inclure les m√©triques analytics (organic_traffic, bounce_rate, etc.)")
):
    """
    R√©cup√©rer les boutiques avec filtres
    
    **Param√®tres disponibles :**
    - `status` : Filtrer par statut de scraping (completed, na, partial, failed)
    - `date_from` : Date de d√©but au format YYYY-MM-DD
    - `date_to` : Date de fin au format YYYY-MM-DD
    - `limit` : Limiter le nombre de r√©sultats
    - `domain` : Filtrer par nom de domaine
    - `include_analytics` : Si true, inclut les m√©triques d√©taill√©es (organic_traffic, bounce_rate, average_visit_duration, branded_traffic, conversion_rate)
    
    **Exemples d'utilisation :**
    - `/shops/filter?status=partial` : Toutes les boutiques avec statut partial
    - `/shops/filter?status=partial&include_analytics=true` : Boutiques partial avec m√©triques
    - `/shops/filter?status=partial&limit=5&include_analytics=true` : 5 premi√®res boutiques partial avec m√©triques
    
    **Exemple de r√©ponse sans analytics :**
    ```json
    {
      "success": true,
      "count": 1,
      "data": [
        {
          "id": 427,
          "domain": "armra.com",
          "name": "ARMRA",
          "scraping_status": "partial",
          "scraping_last_update": "2025-08-04 13:07:59"
        }
      ]
    }
    ```
    
    **Exemple de r√©ponse avec analytics :**
    ```json
    {
      "success": true,
      "count": 1,
      "data": [
        {
          "id": 427,
          "domain": "armra.com",
          "name": "ARMRA",
          "scraping_status": "partial",
          "scraping_last_update": "2025-08-04 13:07:59",
          "organic_traffic": "S√©lecteur non trouv√©",
          "bounce_rate": "45.2%",
          "average_visit_duration": "2m 15s",
          "branded_traffic": "12.5k",
          "conversion_rate": "3.2%"
        }
      ]
    }
    ```
    """
    try:
        all_shops = api.get_all_shops()
        filtered_shops = []
        
        for shop in all_shops:
            # Filtre par status
            if status:
                shop_status = shop.get('scraping_status', '')
                if shop_status != status:
                    continue
            
            # Filtre par domaine
            if domain:
                shop_domain = shop.get('domain', '')
                if domain.lower() not in shop_domain.lower():
                    continue
            
            # Filtre par date
            if date_from:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        from_date = datetime.strptime(date_from, '%Y-%m-%d')
                        if update_date < from_date:
                            continue
                    except:
                        pass
            
            if date_to:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        to_date = datetime.strptime(date_to, '%Y-%m-%d')
                        if update_date > to_date:
                            continue
                    except:
                        pass
            
            # Ajouter les analytics si demand√©
            if include_analytics:
                shop_with_analytics = shop.copy()
                analytics = api.get_shop_analytics(shop.get('id'))
                if analytics:
                    shop_with_analytics.update(analytics)
                else:
                    # Ajouter des champs vides si pas d'analytics
                    shop_with_analytics.update({
                        'organic_traffic': '',
                        'bounce_rate': '',
                        'avg_visit_duration': '',
                        'branded_traffic': '',
                        'conversion_rate': ''
                    })
                filtered_shops.append(shop_with_analytics)
            else:
                filtered_shops.append(shop)
        
        # Limiter les r√©sultats
        if limit:
            filtered_shops = filtered_shops[:limit]
        
        return {
            "success": True,
            "count": len(filtered_shops),
            "filters_applied": {
                "status": status,
                "date_from": date_from,
                "date_to": date_to,
                "limit": limit,
                "domain": domain,
                "include_analytics": include_analytics
            },
            "data": filtered_shops
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur filtrage boutiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('average_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status - accepter 'completed' et 'partial'
        status = target_shop.get('scraping_status')
        if status not in ['completed', 'partial']:
            return {
                "error": f"status = {status}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': '',
            'paid_search_traffic': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('average_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/{shop_id}")
async def get_shop_by_id(shop_id: int):
    """R√©cup√©rer une boutique par ID"""
    try:
        all_shops = api.get_all_shops()
        
        for shop in all_shops:
            if shop.get('id') == shop_id:
                return {
                    "success": True,
                    "data": shop
                }
        
        raise HTTPException(status_code=404, detail=f"Boutique avec ID {shop_id} non trouv√©e")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutique {shop_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/analytics/{shop_id}")
async def get_shop_analytics(shop_id: int):
    """
    R√©cup√©rer les analytics d√©taill√©es d'une boutique
    
    **M√©triques disponibles :**
    - `organic_traffic` : Trafic organique (ex: "12.5k", "S√©lecteur non trouv√©")
    - `bounce_rate` : Taux de rebond (ex: "45.2%", "S√©lecteur non trouv√©")
    - `average_visit_duration` : Dur√©e moyenne de visite (ex: "2m 15s", "S√©lecteur non trouv√©")
    - `branded_traffic` : Trafic de marque (ex: "8.3k", "S√©lecteur non trouv√©")
    - `conversion_rate` : Taux de conversion (ex: "3.2%", "S√©lecteur non trouv√©")
    - `scraping_status` : Statut du scraping (completed, partial, na, failed)
    
    **Exemple de r√©ponse :**
    ```json
    {
      "success": true,
      "shop_id": 427,
      "data": {
        "organic_traffic": "12.5k",
        "bounce_rate": "45.2%",
        "average_visit_duration": "2m 15s",
        "branded_traffic": "8.3k",
        "conversion_rate": "3.2%",
        "scraping_status": "partial"
      }
    }
    ```
    """
    try:
        analytics = api.get_shop_analytics(shop_id)
        
        if analytics:
            return {
                "success": True,
                "shop_id": shop_id,
                "data": analytics
            }
        else:
            raise HTTPException(status_code=404, detail=f"Analytics pour la boutique {shop_id} non trouv√©es")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration analytics {shop_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """R√©cup√©rer les statistiques g√©n√©rales"""
    try:
        all_shops = api.get_all_shops()
        
        # Compter par status
        status_counts = {}
        total_shops = len(all_shops)
        
        for shop in all_shops:
            status = shop.get('scraping_status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        return {
            "success": True,
            "stats": {
                "total_shops": total_shops,
                "status_distribution": status_counts,
                "completion_rate": (status_counts.get('completed', 0) / total_shops * 100) if total_shops > 0 else 0
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur calcul statistiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/export/csv")
async def export_to_csv(
    status: Optional[str] = Query(None, description="Filtrer par status"),
    date_from: Optional[str] = Query(None, description="Date de d√©but (YYYY-MM-DD)"),
    limit: Optional[int] = Query(None, description="Nombre maximum de r√©sultats")
):
    """Exporter les donn√©es en CSV"""
    try:
        import csv
        import io
        
        # R√©cup√©rer les donn√©es filtr√©es
        all_shops = api.get_all_shops()
        filtered_shops = []
        
        for shop in all_shops:
            if status and shop.get('scraping_status') != status:
                continue
            if date_from:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        from_date = datetime.strptime(date_from, '%Y-%m-%d')
                        if update_date < from_date:
                            continue
                    except:
                        pass
            filtered_shops.append(shop)
        
        if limit:
            filtered_shops = filtered_shops[:limit]
        
        # Cr√©er le CSV
        output = io.StringIO()
        if filtered_shops:
            writer = csv.DictWriter(output, fieldnames=filtered_shops[0].keys())
            writer.writeheader()
            writer.writerows(filtered_shops)
        
        csv_content = output.getvalue()
        output.close()
        
        return {
            "success": True,
            "count": len(filtered_shops),
            "csv_data": csv_content
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur export CSV: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status - accepter 'completed' et 'partial'
        status = target_shop.get('scraping_status')
        if status not in ['completed', 'partial']:
            return {
                "error": f"status = {status}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': '',
            'paid_search_traffic': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


# ============================================================================
# ENDPOINTS POST POUR SEM-SCRAPER-FINAL
# ============================================================================

@app.post("/update-shop-analytics")
async def update_shop_analytics_endpoint(request: AnalyticsUpdateRequest):
    """
    Met √† jour les analytics d'une boutique
    
    **Utilis√© par** : sem-scraper-final pour sauvegarder les donn√©es scrap√©es
    
    **Param√®tres** :
    - `shop_id` : ID de la boutique dans la base de donn√©es
    - `analytics_data` : Dictionnaire contenant les m√©triques scrap√©es
    
    **M√©triques support√©es** :
    - `organic_traffic` : Trafic organique (ex: "1,234")
    - `bounce_rate` : Taux de rebond (ex: "45.2%")
    - `average_visit_duration` : Dur√©e moyenne de visite (ex: "2m 30s")
    - `branded_traffic` : Trafic de marque (ex: "567")
    - `conversion_rate` : Taux de conversion (ex: "3.2%")
    - `visits` : Nombre de visites (ex: "5,678")
    - `scraping_status` : Statut du scraping ("completed", "partial", "failed")
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `shop_id` : ID de la boutique mise √† jour
    """
    try:
        success = api.update_shop_analytics(request.shop_id, request.analytics_data)
        if success:
            return {
                "success": True,
                "message": f"Analytics mis √† jour pour shop_id {request.shop_id}",
                "shop_id": request.shop_id
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de la mise √† jour des analytics")
    except Exception as e:
        logger.error(f"‚ùå Erreur mise √† jour analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/mark-shop-failed")
async def mark_shop_failed_endpoint(request: ShopFailureRequest):
    """
    Marque une boutique comme √©chou√©e
    
    **Utilis√© par** : sem-scraper-final quand le scraping √©choue
    
    **Param√®tres** :
    - `shop_id` : ID de la boutique √† marquer comme √©chou√©e
    - `error_message` : Message d'erreur d√©crivant la raison de l'√©chec
    
    **Actions effectu√©es** :
    - Met √† jour le statut de la boutique √† "failed"
    - Enregistre le message d'erreur dans la table scraping_errors
    - Met √† jour la date de derni√®re tentative
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `shop_id` : ID de la boutique marqu√©e comme √©chou√©e
    """
    try:
        success = api.mark_shop_failed(request.shop_id, request.error_message)
        if success:
            return {
                "success": True,
                "message": f"Boutique {request.shop_id} marqu√©e comme √©chou√©e",
                "shop_id": request.shop_id
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec du marquage de la boutique")
    except Exception as e:
        logger.error(f"‚ùå Erreur marquage √©chec boutique: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/record-selector-performance")
async def record_selector_performance_endpoint(request: SelectorPerformanceRequest):
    """
    Enregistre les performances d'un s√©lecteur
    
    **Utilis√© par** : sem-scraper-final pour optimiser les timeouts adaptatifs
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `success` : Boolean indiquant si le s√©lecteur a √©t√© trouv√©
    - `response_time_ms` : Temps de r√©ponse en millisecondes
    - `page_load_time_ms` : Temps de chargement de la page (optionnel)
    
    **Actions effectu√©es** :
    - Enregistre la performance dans la table selector_performance
    - Permet de calculer les timeouts adaptatifs futurs
    - Am√©liore la fiabilit√© du scraping
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `selector_name` : Nom du s√©lecteur enregistr√©
    """
    try:
        success = api.record_selector_performance(
            request.selector_name, 
            request.success, 
            request.response_time_ms,
            request.page_load_time_ms
        )
        if success:
            return {
                "success": True,
                "message": f"Performance s√©lecteur '{request.selector_name}' enregistr√©e",
                "selector_name": request.selector_name
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de l'enregistrement de la performance")
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement performance s√©lecteur: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/get-selector-performances")
async def get_selector_performances_endpoint(request: SelectorPerformanceQuery):
    """
    R√©cup√®re les performances r√©centes d'un s√©lecteur
    
    **Utilis√© par** : sem-scraper-final pour calculer les timeouts adaptatifs
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `limit` : Nombre maximum de performances √† r√©cup√©rer (d√©faut: 20, max: 100)
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `selector_name` : Nom du s√©lecteur demand√©
    - `count` : Nombre de performances r√©cup√©r√©es
    - `data` : Liste des performances avec :
      - `success` : Boolean (succ√®s/√©chec)
      - `response_time_ms` : Temps de r√©ponse
      - `page_load_time_ms` : Temps de chargement de page
      - `timestamp` : Date/heure de la performance
    """
    try:
        performances = api.get_recent_selector_performances(request.selector_name, request.limit)
        return {
            "success": True,
            "selector_name": request.selector_name,
            "count": len(performances),
            "data": performances
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration performances s√©lecteur: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/calculate-adaptive-timeout")
async def calculate_adaptive_timeout_endpoint(request: AdaptiveTimeoutRequest):
    """
    Calcule un timeout adaptatif bas√© sur les performances historiques
    
    **Utilis√© par** : sem-scraper-final pour optimiser les temps d'attente
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `base_timeout` : Timeout de base en millisecondes (d√©faut: 30000, max: 300000)
    
    **Logique de calcul** :
    - Analyse les 15 derni√®res performances du s√©lecteur
    - Calcule le taux de succ√®s et le temps de r√©ponse moyen
    - Ajuste le timeout selon le type de s√©lecteur :
      - Traffic Analysis : 45s-180s
      - Engagement metrics : 30s-120s
      - Organic Search : 25s-120s
      - Branded Traffic : 20s-90s
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `selector_name` : Nom du s√©lecteur
    - `base_timeout` : Timeout de base fourni
    - `adaptive_timeout` : Timeout calcul√© adaptativement
    """
    try:
        timeout = api.calculate_adaptive_timeout(request.selector_name, request.base_timeout)
        return {
            "success": True,
            "selector_name": request.selector_name,
            "base_timeout": request.base_timeout,
            "adaptive_timeout": timeout
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur calcul timeout adaptatif: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status - accepter 'completed' et 'partial'
        status = target_shop.get('scraping_status')
        if status not in ['completed', 'partial']:
            return {
                "error": f"status = {status}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': '',
            'paid_search_traffic': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
#!/usr/bin/env python3
"""
API REST pour les donn√©es TrendTrack
Permet de r√©cup√©rer les donn√©es avec des filtres via HTTP
"""

from fastapi import FastAPI, Query, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime, timezone
import logging
import subprocess
import asyncio

# Import de notre API
from trendtrack_api import TrendTrackAPI, get_database_path

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cr√©er l'application FastAPI
app = FastAPI(
    title="TrendTrack API",
    description="""
    API pour r√©cup√©rer et mettre √† jour les donn√©es de scraping TrendTrack
    
    ## Endpoints GET (Lecture)
    - `/shops` - R√©cup√©rer toutes les boutiques
    - `/shops/with-analytics` - R√©cup√©rer les boutiques avec leurs m√©triques
    - `/shops/filter` - R√©cup√©rer les boutiques avec filtres
    - `/shops/{shop_id}` - R√©cup√©rer une boutique par ID
    - `/analytics/{shop_id}` - R√©cup√©rer les analytics d'une boutique
    - `/stats` - Statistiques g√©n√©rales
    
    ## Endpoints POST (√âcriture - SEM-Scraper)
    - `/update-shop-analytics` - Met √† jour les analytics d'une boutique
    - `/mark-shop-failed` - Marque une boutique comme √©chou√©e
    - `/record-selector-performance` - Enregistre les performances d'un s√©lecteur
    - `/get-selector-performances` - R√©cup√®re les performances r√©centes d'un s√©lecteur
    - `/calculate-adaptive-timeout` - Calcule un timeout adaptatif
    """,
    version="1.0.0"
)

# Ajouter CORS pour permettre les requ√™tes depuis n'importe o√π
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic pour les endpoints POST
class AnalyticsUpdateRequest(BaseModel):
    """
    Mod√®le pour la mise √† jour des analytics d'une boutique
    """
    shop_id: int = Field(..., description="ID de la boutique √† mettre √† jour")
    analytics_data: Dict[str, Any] = Field(..., description="Donn√©es analytics √† sauvegarder", example={
        "organic_traffic": "1,234",
        "bounce_rate": "45.2%",
        "average_visit_duration": "2m 30s",
        "branded_traffic": "567",
        "conversion_rate": "3.2%",
        "visits": "5,678",
        "scraping_status": "completed"
    })

class ShopFailureRequest(BaseModel):
    """
    Mod√®le pour marquer une boutique comme √©chou√©e
    """
    shop_id: int = Field(..., description="ID de la boutique √† marquer comme √©chou√©e")
    error_message: str = Field(..., description="Message d'erreur d√©crivant l'√©chec", example="Timeout lors du scraping des m√©triques Traffic Analysis")

class SelectorPerformanceRequest(BaseModel):
    """
    Mod√®le pour enregistrer les performances d'un s√©lecteur
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    success: bool = Field(..., description="Si le s√©lecteur a √©t√© trouv√© avec succ√®s")
    response_time_ms: int = Field(..., description="Temps de r√©ponse en millisecondes", ge=0)
    page_load_time_ms: Optional[int] = Field(None, description="Temps de chargement de la page en ms", ge=0)

class SelectorPerformanceQuery(BaseModel):
    """
    Mod√®le pour r√©cup√©rer les performances d'un s√©lecteur
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    limit: Optional[int] = Field(20, description="Nombre maximum de performances √† r√©cup√©rer", ge=1, le=100)

class AdaptiveTimeoutRequest(BaseModel):
    """
    Mod√®le pour calculer un timeout adaptatif
    """
    selector_name: str = Field(..., description="Nom du s√©lecteur CSS", example="M√©triques Traffic Analysis")
    base_timeout: Optional[int] = Field(30000, description="Timeout de base en millisecondes", ge=1000, le=300000)

# Initialiser l'API TrendTrack
api = TrendTrackAPI(get_database_path())

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "TrendTrack API",
        "version": "1.0.0",
        "endpoints": {
            "shops": "/shops - R√©cup√©rer toutes les boutiques",
            "shops_filtered": "/shops/filter - R√©cup√©rer les boutiques avec filtres (inclut include_analytics)",
            "shop_by_id": "/shops/{shop_id} - R√©cup√©rer une boutique par ID",
            "analytics": "/analytics/{shop_id} - R√©cup√©rer les analytics d'une boutique",
            "stats": "/stats - Statistiques g√©n√©rales"
        }
    }

@app.get("/shops")
async def get_all_shops():
    """R√©cup√©rer toutes les boutiques"""
    try:
        shops = api.get_all_shops()
        return {
            "success": True,
            "count": len(shops),
            "data": shops
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/with-analytics")
async def get_all_shops_with_analytics():
    """R√©cup√©rer toutes les boutiques avec leurs m√©triques analytics"""
    try:
        all_shops = api.get_all_shops()
        shops_with_analytics = []
        
        for shop in all_shops:
            shop_with_analytics = shop.copy()
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_with_analytics.update(analytics)
            shops_with_analytics.append(shop_with_analytics)
        
        return {
            "success": True,
            "count": len(shops_with_analytics),
            "data": shops_with_analytics
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutiques avec analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/filter")
async def get_filtered_shops(
    status: Optional[str] = Query(None, description="Filtrer par status (completed, na, partial, failed)"),
    date_from: Optional[str] = Query(None, description="Date de d√©but (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="Date de fin (YYYY-MM-DD)"),
    limit: Optional[int] = Query(None, description="Nombre maximum de r√©sultats"),
    domain: Optional[str] = Query(None, description="Filtrer par domaine"),
    include_analytics: Optional[bool] = Query(True, description="Inclure les m√©triques analytics (organic_traffic, bounce_rate, etc.)")
):
    """
    R√©cup√©rer les boutiques avec filtres
    
    **Param√®tres disponibles :**
    - `status` : Filtrer par statut de scraping (completed, na, partial, failed)
    - `date_from` : Date de d√©but au format YYYY-MM-DD
    - `date_to` : Date de fin au format YYYY-MM-DD
    - `limit` : Limiter le nombre de r√©sultats
    - `domain` : Filtrer par nom de domaine
    - `include_analytics` : Si true, inclut les m√©triques d√©taill√©es (organic_traffic, bounce_rate, average_visit_duration, branded_traffic, conversion_rate)
    
    **Exemples d'utilisation :**
    - `/shops/filter?status=partial` : Toutes les boutiques avec statut partial
    - `/shops/filter?status=partial&include_analytics=true` : Boutiques partial avec m√©triques
    - `/shops/filter?status=partial&limit=5&include_analytics=true` : 5 premi√®res boutiques partial avec m√©triques
    
    **Exemple de r√©ponse sans analytics :**
    ```json
    {
      "success": true,
      "count": 1,
      "data": [
        {
          "id": 427,
          "domain": "armra.com",
          "name": "ARMRA",
          "scraping_status": "partial",
          "scraping_last_update": "2025-08-04 13:07:59"
        }
      ]
    }
    ```
    
    **Exemple de r√©ponse avec analytics :**
    ```json
    {
      "success": true,
      "count": 1,
      "data": [
        {
          "id": 427,
          "domain": "armra.com",
          "name": "ARMRA",
          "scraping_status": "partial",
          "scraping_last_update": "2025-08-04 13:07:59",
          "organic_traffic": "S√©lecteur non trouv√©",
          "bounce_rate": "45.2%",
          "average_visit_duration": "2m 15s",
          "branded_traffic": "12.5k",
          "conversion_rate": "3.2%"
        }
      ]
    }
    ```
    """
    try:
        all_shops = api.get_all_shops()
        filtered_shops = []
        
        for shop in all_shops:
            # Filtre par status
            if status:
                shop_status = shop.get('scraping_status', '')
                if shop_status != status:
                    continue
            
            # Filtre par domaine
            if domain:
                shop_domain = shop.get('domain', '')
                if domain.lower() not in shop_domain.lower():
                    continue
            
            # Filtre par date
            if date_from:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        from_date = datetime.strptime(date_from, '%Y-%m-%d')
                        if update_date < from_date:
                            continue
                    except:
                        pass
            
            if date_to:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        to_date = datetime.strptime(date_to, '%Y-%m-%d')
                        if update_date > to_date:
                            continue
                    except:
                        pass
            
            # Ajouter les analytics si demand√©
            if include_analytics:
                shop_with_analytics = shop.copy()
                analytics = api.get_shop_analytics(shop.get('id'))
                if analytics:
                    shop_with_analytics.update(analytics)
                else:
                    # Ajouter des champs vides si pas d'analytics
                    shop_with_analytics.update({
                        'organic_traffic': '',
                        'bounce_rate': '',
                        'avg_visit_duration': '',
                        'branded_traffic': '',
                        'conversion_rate': ''
                    })
                filtered_shops.append(shop_with_analytics)
            else:
                filtered_shops.append(shop)
        
        # Limiter les r√©sultats
        if limit:
            filtered_shops = filtered_shops[:limit]
        
        return {
            "success": True,
            "count": len(filtered_shops),
            "filters_applied": {
                "status": status,
                "date_from": date_from,
                "date_to": date_to,
                "limit": limit,
                "domain": domain,
                "include_analytics": include_analytics
            },
            "data": filtered_shops
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur filtrage boutiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('average_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status
        if target_shop.get('scraping_status') != 'completed':
            return {
                "error": f"status = {target_shop.get('scraping_status')}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('average_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/{shop_id}")
async def get_shop_by_id(shop_id: int):
    """R√©cup√©rer une boutique par ID"""
    try:
        all_shops = api.get_all_shops()
        
        for shop in all_shops:
            if shop.get('id') == shop_id:
                return {
                    "success": True,
                    "data": shop
                }
        
        raise HTTPException(status_code=404, detail=f"Boutique avec ID {shop_id} non trouv√©e")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration boutique {shop_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/analytics/{shop_id}")
async def get_shop_analytics(shop_id: int):
    """
    R√©cup√©rer les analytics d√©taill√©es d'une boutique
    
    **M√©triques disponibles :**
    - `organic_traffic` : Trafic organique (ex: "12.5k", "S√©lecteur non trouv√©")
    - `bounce_rate` : Taux de rebond (ex: "45.2%", "S√©lecteur non trouv√©")
    - `average_visit_duration` : Dur√©e moyenne de visite (ex: "2m 15s", "S√©lecteur non trouv√©")
    - `branded_traffic` : Trafic de marque (ex: "8.3k", "S√©lecteur non trouv√©")
    - `conversion_rate` : Taux de conversion (ex: "3.2%", "S√©lecteur non trouv√©")
    - `scraping_status` : Statut du scraping (completed, partial, na, failed)
    
    **Exemple de r√©ponse :**
    ```json
    {
      "success": true,
      "shop_id": 427,
      "data": {
        "organic_traffic": "12.5k",
        "bounce_rate": "45.2%",
        "average_visit_duration": "2m 15s",
        "branded_traffic": "8.3k",
        "conversion_rate": "3.2%",
        "scraping_status": "partial"
      }
    }
    ```
    """
    try:
        analytics = api.get_shop_analytics(shop_id)
        
        if analytics:
            return {
                "success": True,
                "shop_id": shop_id,
                "data": analytics
            }
        else:
            raise HTTPException(status_code=404, detail=f"Analytics pour la boutique {shop_id} non trouv√©es")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration analytics {shop_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """R√©cup√©rer les statistiques g√©n√©rales"""
    try:
        all_shops = api.get_all_shops()
        
        # Compter par status
        status_counts = {}
        total_shops = len(all_shops)
        
        for shop in all_shops:
            status = shop.get('scraping_status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        return {
            "success": True,
            "stats": {
                "total_shops": total_shops,
                "status_distribution": status_counts,
                "completion_rate": (status_counts.get('completed', 0) / total_shops * 100) if total_shops > 0 else 0
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur calcul statistiques: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/export/csv")
async def export_to_csv(
    status: Optional[str] = Query(None, description="Filtrer par status"),
    date_from: Optional[str] = Query(None, description="Date de d√©but (YYYY-MM-DD)"),
    limit: Optional[int] = Query(None, description="Nombre maximum de r√©sultats")
):
    """Exporter les donn√©es en CSV"""
    try:
        import csv
        import io
        
        # R√©cup√©rer les donn√©es filtr√©es
        all_shops = api.get_all_shops()
        filtered_shops = []
        
        for shop in all_shops:
            if status and shop.get('scraping_status') != status:
                continue
            if date_from:
                last_update = shop.get('scraping_last_update')
                if last_update:
                    try:
                        update_date = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')
                        from_date = datetime.strptime(date_from, '%Y-%m-%d')
                        if update_date < from_date:
                            continue
                    except:
                        pass
            filtered_shops.append(shop)
        
        if limit:
            filtered_shops = filtered_shops[:limit]
        
        # Cr√©er le CSV
        output = io.StringIO()
        if filtered_shops:
            writer = csv.DictWriter(output, fieldnames=filtered_shops[0].keys())
            writer.writeheader()
            writer.writerows(filtered_shops)
        
        csv_content = output.getvalue()
        output.close()
        
        return {
            "success": True,
            "count": len(filtered_shops),
            "csv_data": csv_content
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur export CSV: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status
        if target_shop.get('scraping_status') != 'completed':
            return {
                "error": f"status = {target_shop.get('scraping_status')}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


# ============================================================================
# ENDPOINTS POST POUR SEM-SCRAPER-FINAL
# ============================================================================

@app.post("/update-shop-analytics")
async def update_shop_analytics_endpoint(request: AnalyticsUpdateRequest):
    """
    Met √† jour les analytics d'une boutique
    
    **Utilis√© par** : sem-scraper-final pour sauvegarder les donn√©es scrap√©es
    
    **Param√®tres** :
    - `shop_id` : ID de la boutique dans la base de donn√©es
    - `analytics_data` : Dictionnaire contenant les m√©triques scrap√©es
    
    **M√©triques support√©es** :
    - `organic_traffic` : Trafic organique (ex: "1,234")
    - `bounce_rate` : Taux de rebond (ex: "45.2%")
    - `average_visit_duration` : Dur√©e moyenne de visite (ex: "2m 30s")
    - `branded_traffic` : Trafic de marque (ex: "567")
    - `conversion_rate` : Taux de conversion (ex: "3.2%")
    - `visits` : Nombre de visites (ex: "5,678")
    - `scraping_status` : Statut du scraping ("completed", "partial", "failed")
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `shop_id` : ID de la boutique mise √† jour
    """
    try:
        success = api.update_shop_analytics(request.shop_id, request.analytics_data)
        if success:
            return {
                "success": True,
                "message": f"Analytics mis √† jour pour shop_id {request.shop_id}",
                "shop_id": request.shop_id
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de la mise √† jour des analytics")
    except Exception as e:
        logger.error(f"‚ùå Erreur mise √† jour analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/mark-shop-failed")
async def mark_shop_failed_endpoint(request: ShopFailureRequest):
    """
    Marque une boutique comme √©chou√©e
    
    **Utilis√© par** : sem-scraper-final quand le scraping √©choue
    
    **Param√®tres** :
    - `shop_id` : ID de la boutique √† marquer comme √©chou√©e
    - `error_message` : Message d'erreur d√©crivant la raison de l'√©chec
    
    **Actions effectu√©es** :
    - Met √† jour le statut de la boutique √† "failed"
    - Enregistre le message d'erreur dans la table scraping_errors
    - Met √† jour la date de derni√®re tentative
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `shop_id` : ID de la boutique marqu√©e comme √©chou√©e
    """
    try:
        success = api.mark_shop_failed(request.shop_id, request.error_message)
        if success:
            return {
                "success": True,
                "message": f"Boutique {request.shop_id} marqu√©e comme √©chou√©e",
                "shop_id": request.shop_id
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec du marquage de la boutique")
    except Exception as e:
        logger.error(f"‚ùå Erreur marquage √©chec boutique: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/record-selector-performance")
async def record_selector_performance_endpoint(request: SelectorPerformanceRequest):
    """
    Enregistre les performances d'un s√©lecteur
    
    **Utilis√© par** : sem-scraper-final pour optimiser les timeouts adaptatifs
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `success` : Boolean indiquant si le s√©lecteur a √©t√© trouv√©
    - `response_time_ms` : Temps de r√©ponse en millisecondes
    - `page_load_time_ms` : Temps de chargement de la page (optionnel)
    
    **Actions effectu√©es** :
    - Enregistre la performance dans la table selector_performance
    - Permet de calculer les timeouts adaptatifs futurs
    - Am√©liore la fiabilit√© du scraping
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `message` : Message de confirmation
    - `selector_name` : Nom du s√©lecteur enregistr√©
    """
    try:
        success = api.record_selector_performance(
            request.selector_name, 
            request.success, 
            request.response_time_ms,
            request.page_load_time_ms
        )
        if success:
            return {
                "success": True,
                "message": f"Performance s√©lecteur '{request.selector_name}' enregistr√©e",
                "selector_name": request.selector_name
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de l'enregistrement de la performance")
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement performance s√©lecteur: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/get-selector-performances")
async def get_selector_performances_endpoint(request: SelectorPerformanceQuery):
    """
    R√©cup√®re les performances r√©centes d'un s√©lecteur
    
    **Utilis√© par** : sem-scraper-final pour calculer les timeouts adaptatifs
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `limit` : Nombre maximum de performances √† r√©cup√©rer (d√©faut: 20, max: 100)
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `selector_name` : Nom du s√©lecteur demand√©
    - `count` : Nombre de performances r√©cup√©r√©es
    - `data` : Liste des performances avec :
      - `success` : Boolean (succ√®s/√©chec)
      - `response_time_ms` : Temps de r√©ponse
      - `page_load_time_ms` : Temps de chargement de page
      - `timestamp` : Date/heure de la performance
    """
    try:
        performances = api.get_recent_selector_performances(request.selector_name, request.limit)
        return {
            "success": True,
            "selector_name": request.selector_name,
            "count": len(performances),
            "data": performances
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration performances s√©lecteur: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/calculate-adaptive-timeout")
async def calculate_adaptive_timeout_endpoint(request: AdaptiveTimeoutRequest):
    """
    Calcule un timeout adaptatif bas√© sur les performances historiques
    
    **Utilis√© par** : sem-scraper-final pour optimiser les temps d'attente
    
    **Param√®tres** :
    - `selector_name` : Nom du s√©lecteur CSS (ex: "M√©triques Traffic Analysis")
    - `base_timeout` : Timeout de base en millisecondes (d√©faut: 30000, max: 300000)
    
    **Logique de calcul** :
    - Analyse les 15 derni√®res performances du s√©lecteur
    - Calcule le taux de succ√®s et le temps de r√©ponse moyen
    - Ajuste le timeout selon le type de s√©lecteur :
      - Traffic Analysis : 45s-180s
      - Engagement metrics : 30s-120s
      - Organic Search : 25s-120s
      - Branded Traffic : 20s-90s
    
    **Retour** :
    - `success` : Boolean indiquant le succ√®s de l'op√©ration
    - `selector_name` : Nom du s√©lecteur
    - `base_timeout` : Timeout de base fourni
    - `adaptive_timeout` : Timeout calcul√© adaptativement
    """
    try:
        timeout = api.calculate_adaptive_timeout(request.selector_name, request.base_timeout)
        return {
            "success": True,
            "selector_name": request.selector_name,
            "base_timeout": request.base_timeout,
            "adaptive_timeout": timeout
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur calcul timeout adaptatif: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
@app.get("/shops/completed")
async def get_completed_shops(
    page: int = Query(1, ge=1, description="Num√©ro de page"),
    limit: int = Query(30, ge=1, le=100, description="Nombre d'√©l√©ments par page")
):
    """
    R√©cup√®re la liste pagin√©e des boutiques ayant un status "completed" avec leurs analytics
    """
    try:
        # R√©cup√©rer toutes les boutiques et filtrer par status 'completed'
        all_shops = api.get_all_shops()
        completed_shops = [shop for shop in all_shops if shop.get('scraping_status') == 'completed']
        
        # Pagination
        total = len(completed_shops)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_shops = completed_shops[start_idx:end_idx]
        
        # Ajouter les analytics pour chaque shop
        shops_data = []
        for shop in paginated_shops:
            shop_data = {
                'shop_url': shop.get('shop_url'),
                'organic_traffic': '',
                'bounce_rate': '',
                'avg_visit_duration': '',
                'branded_traffic': '',
                'conversion_rate': ''
            }
            
            # R√©cup√©rer les analytics
            analytics = api.get_shop_analytics(shop.get('id'))
            if analytics:
                shop_data.update({
                    'organic_traffic': analytics.get('organic_traffic', ''),
                    'bounce_rate': analytics.get('bounce_rate', ''),
                    'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                    'branded_traffic': analytics.get('branded_traffic', ''),
                    'conversion_rate': analytics.get('conversion_rate', '')
                })
            
            shops_data.append(shop_data)
        
        return {
            "success": True,
            "data": shops_data,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "totalPages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shops completed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


@app.get("/shops/url/{shop_url:path}")
async def get_shop_by_url(shop_url: str):
    """
    R√©cup√®re les informations d'une boutique sp√©cifique par son URL
    """
    try:
        # R√©cup√©rer toutes les boutiques et chercher par URL
        all_shops = api.get_all_shops()
        target_shop = None
        
        for shop in all_shops:
            if shop.get('shop_url') == shop_url:
                target_shop = shop
                break
        
        if not target_shop:
            raise HTTPException(status_code=404, detail=f"Boutique avec URL {shop_url} non trouv√©e")
        
        # V√©rifier le status
        if target_shop.get('scraping_status') != 'completed':
            return {
                "error": f"status = {target_shop.get('scraping_status')}"
            }
        
        # R√©cup√©rer les analytics
        shop_data = {
            'shop_url': target_shop.get('shop_url'),
            'organic_traffic': '',
            'bounce_rate': '',
            'avg_visit_duration': '',
            'branded_traffic': '',
            'conversion_rate': ''
        }
        
        analytics = api.get_shop_analytics(target_shop.get('id'))
        if analytics:
            shop_data.update({
                'organic_traffic': analytics.get('organic_traffic', ''),
                'bounce_rate': analytics.get('bounce_rate', ''),
                'avg_visit_duration': analytics.get('avg_visit_duration', ''),
                'branded_traffic': analytics.get('branded_traffic', ''),
                'conversion_rate': analytics.get('conversion_rate', '')
            })
        
        return {
            "success": True,
            "data": shop_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration shop par URL {shop_url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es: {str(e)}")


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage de l'API TrendTrack")
    uvicorn.run(app, host="0.0.0.0", port=8000) 