#!/usr/bin/env python3
"""
API TrendTrack adapt√©e pour VPS - VERSION CORRIG√âE (sans doublons)
"""

import sqlite3
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from pathlib import Path
import json

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrendTrackAPI:
    """API pour g√©rer la base de donn√©es TrendTrack - VERSION CORRIG√âE"""
    
    def __init__(self, db_path: str = None):
        """Initialise l'API avec le chemin de la base de donn√©es"""
        if db_path is None:
            # Chemin par d√©faut sur le VPS
            self.db_path = "/home/ubuntu/trendtrack-scraper-final/data/trendtrack.db"
        else:
            self.db_path = db_path
            
        # V√©rifier la structure de la base au d√©marrage
        self._ensure_database_structure()
    
    def _get_connection(self) -> sqlite3.Connection:
        """Obtient une connexion √† la base de donn√©es"""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row  # Permet l'acc√®s par nom de colonne
            return conn
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion base de donn√©es: {e}")
            raise
    
    def _ensure_database_structure(self):
        """V√©rifie et corrige la structure de la base de donn√©es"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # V√©rifier que les tables n√©cessaires existent
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name IN ('shops', 'analytics')
            """)
            required_tables = ['shops', 'analytics']
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            missing_tables = [table for table in required_tables if table not in existing_tables]
            if missing_tables:
                logger.error(f"‚ùå Tables manquantes: {missing_tables}")
                raise Exception(f"Tables manquantes: {missing_tables}")
            
            # V√©rifier la structure de la table analytics
            cursor.execute("PRAGMA table_info(analytics)")
            analytics_columns = [row[1] for row in cursor.fetchall()]
            
            required_columns = [
                'id', 'shop_id', 'organic_traffic', 'bounce_rate', 
                'avg_visit_duration', 'branded_traffic', 'conversion_rate',
                'scraping_status', 'updated_at', 'visits', 'traffic',
                'percent_branded_traffic', 'paid_search_traffic'
            ]
            
            missing_columns = [col for col in required_columns if col not in analytics_columns]
            if missing_columns:
                logger.error(f"‚ùå Colonnes manquantes dans analytics: {missing_columns}")
                raise Exception(f"Colonnes manquantes: {missing_columns}")
            
            # V√©rifier que la contrainte unique sur shop_id existe
            cursor.execute("PRAGMA index_list(analytics)")
            indexes = [row[1] for row in cursor.fetchall()]
            
            if 'idx_analytics_shop_id_unique' not in indexes:
                logger.warning("‚ö†Ô∏è Contrainte unique sur shop_id manquante, ajout...")
                try:
                    cursor.execute("""
                        CREATE UNIQUE INDEX idx_analytics_shop_id_unique 
                        ON analytics(shop_id)
                    """)
                    logger.info("‚úÖ Contrainte unique ajout√©e sur shop_id")
                except Exception as e:
                    logger.error(f"‚ùå Erreur ajout contrainte unique: {e}")
            
            conn.commit()
            logger.info("‚úÖ Structure de la base v√©rifi√©e et corrig√©e")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification structure base: {e}")
            raise
        finally:
            if 'conn' in locals():
                conn.close()
    
    def update_shop_analytics(self, shop_id: int, analytics_data: Dict) -> bool:
        """Met √† jour les analytics d'une boutique - VERSION CORRIG√âE (sans doublons)"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # Pr√©parer les donn√©es
            organic_traffic = analytics_data.get('organic_traffic', '')
            bounce_rate = analytics_data.get('bounce_rate', '')
            avg_visit_duration = analytics_data.get('average_visit_duration', '')
            branded_traffic = analytics_data.get('branded_traffic', '')
            conversion_rate = analytics_data.get('conversion_rate', '')
            paid_search_traffic = analytics_data.get('paid_search_traffic', '')
            visits = analytics_data.get('visits', '')
            traffic = analytics_data.get('traffic', '')
            percent_branded_traffic = analytics_data.get('percent_branded_traffic', '')
            
            # D√©terminer le statut automatiquement
            scraping_status = analytics_data.get('scraping_status', 'completed')
            
            # Si un des champs contient "S√©lecteur non trouv√©" ou est vide, mettre le statut √† 'partial'
            selector_not_found = 'S√©lecteur non trouv√©'
            required_fields = [organic_traffic, bounce_rate, avg_visit_duration, branded_traffic, conversion_rate, visits, traffic, percent_branded_traffic]
            
            if (any(field == selector_not_found for field in required_fields) or 
                any(field == '' or field is None for field in required_fields)):
                scraping_status = 'partial'
            
            # V√âRIFICATION CRITIQUE : V√©rifier si l'entr√©e existe d√©j√†
            cursor.execute("SELECT id FROM analytics WHERE shop_id = ?", (shop_id,))
            existing = cursor.fetchone()
            
            if existing:
                # MISE √Ä JOUR : L'entr√©e existe, on la met √† jour
                logger.info(f"üîÑ Mise √† jour analytics existante pour shop_id {shop_id}")
                cursor.execute("""
                    UPDATE analytics SET
                    organic_traffic = ?, bounce_rate = ?, avg_visit_duration = ?,
                    branded_traffic = ?, conversion_rate = ?, paid_search_traffic = ?,
                    visits = ?, traffic = ?, percent_branded_traffic = ?,
                    scraping_status = ?, updated_at = ?
                    WHERE shop_id = ?
                """, (organic_traffic, bounce_rate, avg_visit_duration, branded_traffic,
                      conversion_rate, paid_search_traffic, visits, traffic, 
                      percent_branded_traffic, scraping_status, datetime.now(timezone.utc), shop_id))
                
                rows_updated = cursor.rowcount
                if rows_updated == 0:
                    logger.warning(f"‚ö†Ô∏è Aucune ligne mise √† jour pour shop_id {shop_id}")
                else:
                    logger.info(f"‚úÖ {rows_updated} ligne(s) mise(s) √† jour pour shop_id {shop_id}")
            else:
                # INSERTION : L'entr√©e n'existe pas, on l'ins√®re
                logger.info(f"‚ûï Insertion nouvelle analytics pour shop_id {shop_id}")
                cursor.execute("""
                    INSERT INTO analytics 
                    (shop_id, organic_traffic, bounce_rate, avg_visit_duration, branded_traffic,
                     conversion_rate, paid_search_traffic, visits, traffic, percent_branded_traffic,
                     scraping_status, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (shop_id, organic_traffic, bounce_rate, avg_visit_duration, branded_traffic,
                      conversion_rate, paid_search_traffic, visits, traffic, 
                      percent_branded_traffic, scraping_status, datetime.now(timezone.utc)))
                
                logger.info(f"‚úÖ Nouvelle analytics ins√©r√©e pour shop_id {shop_id}")
            
            # Mettre √† jour le status dans la table shops
            cursor.execute("""
                UPDATE shops 
                SET scraping_status = ?, scraping_last_update = ?
                WHERE id = ?
            """, (scraping_status, datetime.now(timezone.utc), shop_id))
            
            conn.commit()
            logger.info(f"‚úÖ Analytics mis √† jour pour shop_id {shop_id} (statut: {scraping_status})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour analytics shop_id {shop_id}: {e}")
            if 'conn' in locals():
                conn.rollback()
            return False
        finally:
            if 'conn' in locals():
                conn.close()
    
    def get_shop_analytics(self, shop_id: int) -> Optional[Dict]:
        """R√©cup√®re les analytics d'une boutique - VERSION CORRIG√âE"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # R√©cup√©rer les analytics les plus r√©cents pour cette boutique
            cursor.execute("""
                SELECT organic_traffic, bounce_rate, avg_visit_duration, branded_traffic, 
                       conversion_rate, visits, traffic, percent_branded_traffic, scraping_status,
                       paid_search_traffic, updated_at
                FROM analytics 
                WHERE shop_id = ?
                ORDER BY updated_at DESC
                LIMIT 1
            """, (shop_id,))
            
            row = cursor.fetchone()
            if row:
                return {
                    'organic_traffic': row[0],
                    'bounce_rate': row[1],
                    'average_visit_duration': row[2],
                    'branded_traffic': row[3],
                    'conversion_rate': row[4],
                    'visits': row[5],
                    'traffic': row[6],
                    'percent_branded_traffic': row[7],
                    'scraping_status': row[8],
                    'paid_search_traffic': row[9],
                    'updated_at': row[10]
                }
            else:
                logger.info(f"‚ÑπÔ∏è Aucune analytics trouv√©e pour shop_id {shop_id}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration analytics shop_id {shop_id}: {e}")
            return None
        finally:
            if 'conn' in locals():
                conn.close()
    
    def get_all_shops(self) -> List[Dict]:
        """R√©cup√®re toutes les boutiques"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT id, shop_name, shop_url, scraping_status, scraping_last_update,
                       creation_date, monthly_visits, monthly_revenue, live_ads,
                       page_number, scraped_at, project_source, external_id, metadata, updated_at
                FROM shops
                ORDER BY id
            """)
            
            shops = []
            for row in cursor.fetchall():
                shop = dict(row)
                shops.append(shop)
            
            logger.info(f"‚úÖ {len(shops)} boutiques r√©cup√©r√©es")
            return shops
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration boutiques: {e}")
            return []
        finally:
            if 'conn' in locals():
                conn.close()
    
    def get_all_shops_with_analytics(self, limit: int = None) -> List[Dict]:
        """R√©cup√®re toutes les boutiques avec leurs m√©triques analytics - VERSION CORRIG√âE"""
        try:
            all_shops = self.get_all_shops()
            shops_with_analytics = []
            
            for shop in all_shops:
                shop_with_analytics = shop.copy()
                analytics = self.get_shop_analytics(shop.get('id'))
                if analytics:
                    shop_with_analytics.update(analytics)
                shops_with_analytics.append(shop_with_analytics)
            
            # Trier par qualit√© des donn√©es (boutiques compl√®tes en premier)
            shops_with_analytics.sort(key=lambda x: x.get('scraping_status', ''), reverse=True)
            
            if limit:
                shops_with_analytics = shops_with_analytics[:limit]
            
            logger.info(f"‚úÖ {len(shops_with_analytics)} boutiques avec analytics r√©cup√©r√©es")
            return shops_with_analytics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration boutiques avec analytics: {e}")
            return []
    
    def mark_shop_failed(self, shop_id: int, error_message: str) -> bool:
        """Marque une boutique comme √©chou√©e"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # Mettre √† jour le statut dans shops
            cursor.execute("""
                UPDATE shops 
                SET scraping_status = 'failed', scraping_last_update = ?
                WHERE id = ?
            """, (datetime.now(timezone.utc), shop_id))
            
            # Ajouter une entr√©e dans analytics avec le statut 'failed'
            cursor.execute("""
                INSERT INTO analytics 
                (shop_id, scraping_status, updated_at)
                VALUES (?, 'failed', ?)
            """, (shop_id, datetime.now(timezone.utc)))
            
            conn.commit()
            logger.info(f"‚úÖ Shop {shop_id} marqu√© comme √©chou√©: {error_message}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur marquage shop {shop_id} comme √©chou√©: {e}")
            if 'conn' in locals():
                conn.rollback()
            return False
        finally:
            if 'conn' in locals():
                conn.close()

    def calculate_adaptive_timeout(self, selector_name: str, base_timeout: int = 30000) -> int:
        """Calcule un timeout adaptatif bas√© sur l'historique des performances"""
        try:
            # Pour l'instant, retourner le timeout de base
            # TODO: Impl√©menter la logique adaptative compl√®te
            return base_timeout
                
        except Exception as e:
            # En cas d'erreur, retourner le timeout de base
            return base_timeout

    def get_selector_success_rate(self, selector_name: str) -> float:
        """R√©cup√®re le taux de succ√®s d'un s√©lecteur"""
        try:
            # Pour l'instant, retourner un taux par d√©faut
            return 0.8  # 80% de succ√®s par d√©faut
        except Exception as e:
            return 0.8

    def reset_selector_performance(self):
        """R√©initialise les performances des s√©lecteurs"""
        try:
            # Pour l'instant, ne rien faire
            pass
        except Exception as e:
            pass

    def record_selector_performance(self, selector_name: str, success: bool, response_time: int):
        """Enregistre la performance d'un s√©lecteur"""
        try:
            # Pour l'instant, ne rien faire
            pass
        except Exception as e:
            pass

    def acquire_lock(self, timeout: int = 30) -> bool:
        """Acquiert un verrou pour le scraping"""
        try:
            # Pour l'instant, toujours r√©ussir
            return True
        except Exception as e:
            return True

    def release_lock(self):
        """Lib√®re le verrou de scraping"""
        try:
            # Pour l'instant, ne rien faire
            pass
        except Exception as e:
            pass

    def is_shop_eligible_for_scraping(self, shop: dict) -> bool:
        """V√©rifie si une boutique est √©ligible pour le scraping"""
        try:
            # Pour l'instant, toutes les boutiques sont √©ligibles
            return True
        except Exception as e:
            return True

# Instance globale de l'API
api = TrendTrackAPI()

# Fonction de test pour validation
def test_api_functionality():
    """Teste la fonctionnalit√© de l'API corrig√©e"""
    try:
        print("üß™ TEST DE L'API CORRIG√âE")
        print("=" * 40)
        
        # Test 1: V√©rifier la structure
        print("‚úÖ Structure de la base v√©rifi√©e")
        
        # Test 2: R√©cup√©rer quelques boutiques
        shops = api.get_all_shops_with_analytics(limit=3)
        print(f"‚úÖ {len(shops)} boutiques r√©cup√©r√©es avec analytics")
        
        # Test 3: V√©rifier qu'il n'y a pas de doublons
        if shops:
            shop_ids = [shop['id'] for shop in shops]
            unique_ids = set(shop_ids)
            if len(shop_ids) == len(unique_ids):
                print("‚úÖ Aucun doublon d√©tect√©")
            else:
                print("‚ùå Doublons d√©tect√©s!")
        
        print("üéØ API corrig√©e fonctionne parfaitement!")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur test API: {e}")
        return False

    def get_all_analytics(self):
        """
        R√©cup√®re toutes les m√©triques analytics de la base de donn√©es
        """
        try:
            cursor = self.conn.execute("""
                SELECT id, shop_id, metric_type, metric_value, scraping_status, 
                       scraping_date, created_at, updated_at
                FROM analytics 
                ORDER BY shop_id, metric_type, created_at DESC
            """)
            
            analytics = []
            for row in cursor.fetchall():
                analytics.append({
                    "id": row[0],
                    "shop_id": row[1],
                    "metric_type": row[2],
                    "metric_value": row[3],
                    "scraping_status": row[4],
                    "scraping_date": row[5],
                    "created_at": row[6],
                    "updated_at": row[7]
                })
            
            self.logger.info(f"‚úÖ {len(analytics)} m√©triques analytics r√©cup√©r√©es")
            return analytics
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des analytics: {e}")
            return []

if __name__ == "__main__":
    test_api_functionality()
