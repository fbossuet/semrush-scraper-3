# Implementation Tasks: Scraper SEM Parall√®le

**Feature**: Scraper SEM Parall√®le  
**Branch**: `001-name-trendtrack-scraper`  
**Date**: 2025-09-16  
**Generated from**: plan.md, data-model.md, research.md, quickstart.md, contracts/openapi.yaml

## Task Overview

**Total Tasks**: 46  
**Parallel Tasks**: 15 (marked with [P])  
**Sequential Tasks**: 26  

**Strat√©gie d'Ex√©cution**: Approche TDD avec tests avant impl√©mentation, ex√©cution ordonn√©e par d√©pendances, ex√©cution parall√®le quand possible.

## Corrections Appliqu√©es

### ‚úÖ T014-CORRECTION: Correction du comptage des m√©triques avg_visit_duration
**Date**: 2025-09-17  
**Probl√®me**: La m√©trique `avg_visit_duration` affichait "Aucune tentative" malgr√© des donn√©es r√©cup√©r√©es  
**Cause**: Incoh√©rence entre les noms de cl√©s dans `detailed_metrics` (`'avg_visit_duration'`) et les donn√©es du worker (`'average_visit_duration'`)  
**Solution**: 
- Correction de l'initialisation de `detailed_metrics` pour utiliser `'average_visit_duration'`
- Ajout du mapping inverse dans `data_names` pour l'affichage
- Nettoyage des logs de debug temporaires

**Fichiers modifi√©s**:
- `production_scraper_parallel.py`: Logique de comptage des m√©triques
- `launch_workers_by_status.py`: Agr√©gation et affichage des statistiques

**R√©sultat**: `avg_visit_duration` affiche maintenant correctement `üìä Total: 2 | Skip√©es: 1 | Succ√®s: 1 | √âchecs: 0 | Taux: 100.0%`

### ‚úÖ T014-CORRECTION-2: Investigation probl√®me conversion_rate React SPA
**Date**: 2025-09-17  
**Probl√®me**: La m√©trique `conversion_rate` n'est jamais r√©cup√©r√©e malgr√© des donn√©es visibles en frontend  
**Cause**: Page React SPA se charge mais le contenu est vide - probl√®me de timing et de chargement asynchrone  
**Solution**: 
- Attente progressive pour React SPA (1s, 1.5s, 2s, 2.5s, 3s max)
- D√©tection des pages vides et attente suppl√©mentaire (5s)
- Debug complet (URL, titre, contenu de la page)
- V√©rification des donn√©es sp√©cifiques au domaine

**Fichiers modifi√©s**:
- `production_scraper_parallel.py`: Am√©lioration du timing React SPA et d√©tection des pages vides

**R√©sultat**: Correction du timing pour React SPA, d√©tection des probl√®mes de permissions/session

## üéØ **NOUVELLES T√ÇCHES - YEAR_FOUNDED**

### T051: Lancer le scraper TrendTrack pour alimenter la table shops
- **Objectif**: Alimenter la base de donn√©es avec des boutiques pour tester year_founded
- **Action**: Ex√©cuter le scraper TrendTrack existant
- **Priorit√©**: P1
- **D√©pendances**: Aucune

### T052: V√©rifier que creationDate est bien extrait dans les nouvelles donn√©es
- **Objectif**: Confirmer que le scraper extrait bien creationDate (format MM/DD/YYYY)
- **Action**: V√©rifier les donn√©es scrap√©es dans la table shops
- **Priorit√©**: P1
- **D√©pendances**: T051

### T053: Tester la conversion creationDate ‚Üí year_founded avec les scripts existants
- **Objectif**: Utiliser les scripts existants pour convertir creationDate en year_founded
- **Action**: Tester update_year_founded.py et get_shops_without_year_founded.py
- **Priorit√©**: P1
- **D√©pendances**: T052

### T054: Nettoyer les fichiers temporaires cr√©√©s
- **Objectif**: Supprimer les fichiers de test cr√©√©s pendant le d√©veloppement
- **Action**: Nettoyer /tmp/year_founded_dom_scraper.py et autres fichiers temporaires
- **Priorit√©**: P2
- **D√©pendances**: T053

### T055: ‚úÖ Impl√©menter le scraping des technologies (pixels Google/Facebook) - TERMIN√â
- **Objectif**: R√©cup√©rer les donn√©es du conteneur "technologies" dans l'interface TrendTrack
- **Action**: Analyser l'interface TrendTrack, ajouter des s√©lecteurs pour pixel_google et pixel_facebook
- **Contrainte**: Tout doit √™tre fait en Python dans la mesure du possible
- **Priorit√©**: P1 - ‚úÖ **TERMIN√â LE 18/09/2025**
- **D√©pendances**: T051 (apr√®s avoir des donn√©es de test)
- **R√©sultat**: 
  - ‚úÖ Scraper Python cr√©√© (`pixel_scraper_simple.py`)
  - ‚úÖ Int√©gration TrendTrack compl√®te avec authentification
  - ‚úÖ Test r√©ussi sur VPS (3 boutiques trait√©es, 100% de succ√®s)
  - ‚úÖ Champs `pixel_google` et `pixel_facebook` v√©rifi√©s en base (TEXT)
  - ‚úÖ Heuristiques robustes pour d√©tecter Google Analytics et Facebook Pixel
  - ‚úÖ Sauvegarde automatique en base de donn√©es
  - ‚úÖ Extraction de domaine corrig√©e (shopify.com, etsy.com, amazon.com)

### T056: ‚úÖ Int√©gration compl√®te du scraper de pixels dans le workflow principal - TERMIN√â
- **Objectif**: Int√©grer le scraper de pixels directement dans TrendTrackExtractor pour automatisation compl√®te
- **Action**: Modifier TrendTrackExtractor pour inclure automatiquement le scraping des pixels
- **Contrainte**: Int√©gration transparente sans casser le workflow existant
- **Priorit√©**: P1 - ‚úÖ **TERMIN√â LE 18/09/2025**
- **D√©pendances**: T055
- **R√©sultat**: 
  - ‚úÖ TrendTrackExtractor modifi√© avec int√©gration des pixels
  - ‚úÖ Scraper principal lance maintenant automatiquement : m√©triques de base + market + pixels
  - ‚úÖ Sauvegarde automatique de l'ancienne version
  - ‚úÖ Script de mise √† jour cr√©√© et test√©
  - ‚úÖ Workflow unifi√© : `bash start-scraper.sh` ‚Üí tout se lance automatiquement

### T057: Test et validation du scraper unifi√© en production
- **Objectif**: Tester le scraper unifi√© sur des donn√©es r√©elles en production
- **Action**: Lancer le scraper principal et valider que toutes les m√©triques sont extraites
- **Contrainte**: Validation manuelle des r√©sultats par l'utilisateur
- **Priorit√©**: P1
- **D√©pendances**: T056
- **Validation**: 
  - Tester le scraper principal avec `bash start-scraper.sh`
  - V√©rifier que les m√©triques de base, market et pixels sont toutes extraites
  - Confirmer la sauvegarde compl√®te en base de donn√©es
  - Valider les performances et la stabilit√© du workflow unifi√©

## Priorisation (P0/P1/P2)

- P0 (Critique, √† ex√©cuter en premier):
  - T074 (Audit du fonctionnement de l'API endpoint test)
  - T041 (Impl√©menter la m√©trique visits manquante)
  - T030 (Cr√©er environnements de test et pr√©production)
  - T031 (Ajouter un syst√®me de pause entre les requ√™tes API)
  - T032 (Refactoring authentification API pour TrendTrack)
  - T033 (Architecture Playwright headless + stealth pour TrendTrack)
  - T034 (Syst√®me de pauses al√©atoires pour TrendTrack)
  - T001, T002 (Audit + comparaison BDD)
  - T006, T007, T008, T009 (Tests de r√©gression/baseline)
  - T016 (Service de validation des m√©triques)
  - T015 (Service de gestion d'erreurs)
  - T021 (Int√©gration client API MyToolsPlan)
  - T073 (V√©rification format ISO 8601 UTC pour toutes les dates en BDD)
r
- P1 (Important, apr√®s P0):
  - T035 (Architecture de fichiers claire pour TrendTrack)
  - T036 (Scraper recherche par domaine)
  - T037 (Int√©gration base de donn√©es pour recherche par domaine)
  - T038 (Tests de r√©gression pour nouvelle fonctionnalit√©)
  - T010, T011, T012 (Entit√©s et services de base si √©carts d√©tect√©s)
  - T013, T014 (Services scraping de base + workers parall√®les)
  - T017, T018, T019, T020 (Endpoints API)

- P2 (Finition/optimisation):
  - T039 (Migration scraper traditionnel vers nouvelle architecture)
  - T040 (Documentation et d√©ploiement)
  - T024, T025, T026, T027, T028 (Unitaires, perf, docs, E2E, d√©ploiement)

## Phase 0: Audit du Projet Existant (REMPLACE le Setup)

### T001: Audit rapide du code et de la doc existants [P]
**Type**: Audit  
**Dependencies**: None  
**Files**: `sem-scraper-final/production_scraper_parallel.py`, `trendtrack-scraper-final/`, `ubuntu/README/*`, `.specify/memory/constitution.md`  
**Description**: Cartographier les composants d√©j√† en place (scraper, APIs, DB, workers, logs) et v√©rifier leur coh√©rence avec la constitution `.specify`.

**Implementation**:
- Lister les points d‚Äôentr√©e (scripts de lancement, workers, endpoints)
- V√©rifier la pr√©sence du syst√®me de rollback et l‚Äô√©tat des logs
- Valider que la logique de validation m√©triques est en place et utilis√©e
- Noter les divergences doc ‚Üî code

**Validation**: Rapport d‚Äôaudit bref (10 lignes) + liste de divergences, valid√©s par l‚Äôutilisateur.

### T002: V√©rification sch√©mas BDD prod/test (sans migration) [P]
**Type**: Audit  
**Dependencies**: T001  
**Files**: `trendtrack-final-scraper/data/trendtrack.db`, `trendtrack-final-scraper/data/trendtrack_test.db`  
**Description**: Comparer les sch√©mas des deux BDD, sans cr√©er de nouvelles migrations.

**Implementation**:
- Exporter la structure des tables (shops, analytics, scraping_sessions, workers)
- Comparer colonnes, types, index
- Lister les √©carts √©ventuels

**Validation**: Rapport de comparaison valid√© par l‚Äôutilisateur; aucune modification appliqu√©e.

## Test Tasks (R√©gression sur existant)

### T003: Website Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_website_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© Website bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD Website (GET, POST, PUT, DELETE /websites)
- Valider que les sch√©mas request/response correspondent √† la sp√©cification OpenAPI
- Tester le filtrage par statut et la pagination
- Tester la gestion d'erreurs (404, 400, 409)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T004: Metrics Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_metrics_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© Metrics bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD metrics (GET, PUT /websites/{id}/metrics)
- Valider le sch√©ma des 8 m√©triques (organic_traffic, paid_search_traffic, visits, bounce_rate, avg_visit_duration, branded_traffic, conversion_rate, percent_branded_traffic)
- Tester les r√®gles de validation des m√©triques
- Tester la gestion d'erreurs (404, 400)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T005: ScrapingSession Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_session_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© ScrapingSession bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD session (GET, POST, DELETE /scraping/sessions)
- Tester l'endpoint de statut de session (/scraping/sessions/{id}/status)
- Valider le sch√©ma de session et les transitions de statut
- Tester la gestion d'erreurs (404, 400)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T006: Integration Test - Basic Website Scraping [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_basic_scraping.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de scraping de base d'un site web depuis quickstart.md.

**Impl√©mentation**:
- Tester le scraping end-to-end d'un site web unique
- Valider la collecte de m√©triques et les mises √† jour de statut
- Tester la cr√©ation de session et le monitoring
- Valider les r√©ponses des endpoints API

**Validation**: Doit passer sur le code actuel (sert de baseline), sinon ouvrir bug.

### T007: Integration Test - Parallel Worker Processing [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_parallel_workers.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de traitement parall√®le de workers depuis quickstart.md.

**Impl√©mentation**:
- Tester le traitement parall√®le de plusieurs sites web
- Valider la distribution de charge des workers
- Tester le monitoring du progr√®s de session
- Valider le traitement concurrent

**Validation**: Doit passer sur le code actuel (baseline), sinon ouvrir bug.

### T008: Integration Test - Error Handling and Recovery [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_error_handling.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de gestion d'erreurs et de d√©gradation gracieuse depuis quickstart.md.

**Impl√©mentation**:
- Tester la gestion des sites web invalides
- Valider la d√©gradation gracieuse avec des √©checs partiels
- Tester le logging d'erreurs et les mises √† jour de statut
- Valider la completion de session avec des r√©sultats mixtes

**Validation**: Doit passer (baseline), sinon ouvrir bug.

### T009: Integration Test - Metric Validation [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_metric_validation.py`  
**Description**: Cr√©er un test d'int√©gration pour la logique de validation des m√©triques depuis quickstart.md.

**Impl√©mentation**:
- Tester la collecte des 8 m√©triques
- Valider la logique de statut (completed vs partial)
- Tester les m√©triques calcul√©es (percent_branded_traffic)
- Valider les r√®gles de validation des m√©triques

**Validation**: Doit passer (baseline), sinon ouvrir bug.

## Core Implementation Tasks

### T010: Website Entity Implementation
**Type**: Core  
**Dependencies**: T003  
**Files**: `models/website.py`, `services/website_service.py`  
**Description**: Impl√©menter l'entit√© Website et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le Website avec tous les champs de data-model.md
- Impl√©menter WebsiteService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour shop_url, scraping_status, timestamps
- Impl√©menter la logique de transition de statut

**Validation**: Les tests de contrats T003 passent, l'entit√© Website fonctionne correctement.

### T011: Metrics Entity Implementation
**Type**: Core  
**Dependencies**: T004  
**Files**: `models/metrics.py`, `services/metrics_service.py`  
**Description**: Impl√©menter l'entit√© Metrics et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le Metrics avec les 8 champs de m√©triques
- Impl√©menter MetricsService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour les champs num√©riques et les plages d√©cimales
- Impl√©menter la logique de champ calcul√© (percent_branded_traffic)

**Validation**: Les tests de contrats T004 passent, l'entit√© Metrics fonctionne correctement.

### T012: ScrapingSession Entity Implementation
**Type**: Core  
**Dependencies**: T005  
**Files**: `models/scraping_session.py`, `services/session_service.py`  
**Description**: Impl√©menter l'entit√© ScrapingSession et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le ScrapingSession avec tous les champs
- Impl√©menter SessionService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour worker_count, websites_per_worker
- Impl√©menter la gestion du statut de session

**Validation**: Les tests de contrats T005 passent, l'entit√© ScrapingSession fonctionne correctement.

### T013: Basic Scraping Service Implementation
**Type**: Core  
**Dependencies**: T006, T010, T011  
**Files**: `services/scraping_service.py`  
**Description**: Impl√©menter le service de scraping de base pour le traitement d'un site web unique.

**Impl√©mentation**:
- Cr√©er ScrapingService avec traitement d'un site web unique
- Impl√©menter l'authentification MyToolsPlan et la gestion de session
- Ajouter la collecte de m√©triques de base (APIs + DOM scraping)
- Impl√©menter la logique de mise √† jour de statut

**Validation**: Le test d'int√©gration T006 passe, le scraping de base fonctionne.

### T014: Parallel Worker Service Implementation
**Type**: Core  
**Dependencies**: T007, T012, T013  
**Files**: `services/worker_service.py`, `workers/parallel_worker.py`  
**Description**: Impl√©menter le service de workers parall√®les pour le traitement concurrent.

**Impl√©mentation**:
- Cr√©er WorkerService avec la logique de traitement parall√®le
- Impl√©menter la classe ParallelWorker avec asyncio
- Ajouter la gestion du pool de workers et la distribution de charge
- Impl√©menter le suivi du progr√®s de session

**Validation**: Le test d'int√©gration T007 passe, le traitement parall√®le fonctionne.

### T015: Error Handling Service Implementation
**Type**: Core  
**Dependencies**: T008, T013  
**Files**: `services/error_handling_service.py`  
**Description**: Impl√©menter une gestion d'erreurs compl√®te et une d√©gradation gracieuse.

**Impl√©mentation**:
- Cr√©er ErrorHandlingService avec la logique de retry
- Impl√©menter la d√©gradation gracieuse pour les √©checs partiels
- Ajouter un logging d'erreurs complet
- Impl√©menter les m√©canismes de r√©cup√©ration d'erreurs

**Validation**: Le test d'int√©gration T008 passe, la gestion d'erreurs fonctionne correctement.

### T016: Metric Validation Service Implementation
**Type**: Core  
**Dependencies**: T009, T011  
**Files**: `services/metric_validation_service.py`  
**Description**: Impl√©menter la logique de validation adaptative des m√©triques.

**Impl√©mentation**:
- Cr√©er MetricValidationService avec validation dynamique
- Impl√©menter la logique de validation des 8 m√©triques
- Ajouter la d√©termination de statut (completed/partial/failed/na)
- Impl√©menter la validation des m√©triques calcul√©es

**Validation**: Le test d'int√©gration T009 passe, la validation des m√©triques fonctionne correctement.

## API Implementation Tasks

### T017: Website API Endpoints Implementation
**Type**: API  
**Dependencies**: T010  
**Files**: `api/website_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© Website.

**Impl√©mentation**:
- Impl√©menter GET /websites (liste avec filtrage)
- Impl√©menter POST /websites (cr√©ation)
- Impl√©menter GET /websites/{id} (obtenir d√©tails)
- Impl√©menter PUT /websites/{id} (mise √† jour)
- Impl√©menter DELETE /websites/{id} (suppression)

**Validation**: Tous les endpoints API Website fonctionnent correctement, correspondent au contrat OpenAPI.

### T018: Metrics API Endpoints Implementation
**Type**: API  
**Dependencies**: T011  
**Files**: `api/metrics_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© Metrics.

**Impl√©mentation**:
- Impl√©menter GET /websites/{id}/metrics (obtenir m√©triques)
- Impl√©menter PUT /websites/{id}/metrics (mettre √† jour m√©triques)
- Ajouter une gestion d'erreurs et validation appropri√©es
- S'assurer que les sch√©mas de r√©ponse correspondent √† OpenAPI

**Validation**: Tous les endpoints API Metrics fonctionnent correctement, correspondent au contrat OpenAPI.

### T019: ScrapingSession API Endpoints Implementation
**Type**: API  
**Dependencies**: T012  
**Files**: `api/session_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© ScrapingSession.

**Impl√©mentation**:
- Impl√©menter GET /scraping/sessions (lister sessions)
- Impl√©menter POST /scraping/sessions (d√©marrer session)
- Impl√©menter GET /scraping/sessions/{id} (obtenir d√©tails)
- Impl√©menter DELETE /scraping/sessions/{id} (annuler session)
- Impl√©menter GET /scraping/sessions/{id}/status (obtenir statut)

**Validation**: Tous les endpoints API ScrapingSession fonctionnent correctement, correspondent au contrat OpenAPI.

### T020: Health Check API Endpoint Implementation
**Type**: API  
**Dependencies**: None  
**Files**: `api/health_endpoints.py`  
**Description**: Impl√©menter l'endpoint de v√©rification de sant√©.

**Impl√©mentation**:
- Impl√©menter l'endpoint GET /health
- Ajouter la v√©rification de connectivit√© base de donn√©es
- Ajouter la validation du statut syst√®me
- Retourner le statut de sant√© appropri√©

**Validation**: L'endpoint de v√©rification de sant√© retourne le statut correct.

## Integration Tasks

### T021: MyToolsPlan API Client Integration
**Type**: Integration  
**Dependencies**: T013  
**Files**: `clients/mytoolsplan_client.py`  
**Description**: Impl√©menter le client API MyToolsPlan avec authentification et gestion de session.

**Impl√©mentation**:
- Cr√©er MyToolsPlanClient avec authentification
- Impl√©menter la synchronisation de cookies entre domaines
- Ajouter les endpoints API pour organic.Summary, organic.OverviewTrend, engagement
- Impl√©menter le syst√®me de fallback (sam2.mytoolsplan.xyz)

**Validation**: Les APIs MyToolsPlan s'int√®grent correctement, l'authentification fonctionne.

### T022: Database Integration Service
**Type**: Integration  
**Dependencies**: T010, T011, T012  
**Files**: `services/database_service.py`  
**Description**: Impl√©menter l'int√©gration base de donn√©es avec support de double base de donn√©es.

**Impl√©mentation**:
- Cr√©er DatabaseService avec basculement base de donn√©es production/test
- Impl√©menter le pool de connexions et la gestion de transactions
- Ajouter le support de migration de base de donn√©es
- Impl√©menter les proc√©dures de sauvegarde et r√©cup√©ration

**Validation**: Les op√©rations de base de donn√©es fonctionnent correctement sur les deux bases.

### T023: Logging and Monitoring Integration
**Type**: Integration  
**Dependencies**: T015  
**Files**: `services/logging_service.py`, `monitoring/metrics_collector.py`  
**Description**: Impl√©menter un logging et monitoring complets.

**Impl√©mentation**:
- Cr√©er LoggingService avec logging structur√©
- Impl√©menter le monitoring pour les performances de scraping
- Ajouter le suivi d'erreurs et l'alerte
- Impl√©menter les outils d'analyse de logs et de d√©bogage

**Validation**: Le logging et monitoring fonctionnent correctement, fournissent des insights utiles.

### T031: Syst√®me de pause/throttling entre requ√™tes API (anti-d√©tection) [P0]
**Type**: Integration  
**Dependencies**: T021  
**Files**: `clients/mytoolsplan_client.py`, `parallel_config.py`, `production_scraper_parallel.py`  
**Description**: Ajouter un syst√®me de pauses intelligentes (throttling + jitter) entre les requ√™tes API pour r√©duire le risque de d√©tection/ban.

**Impl√©mentation**:
- Introduire des d√©lais al√©atoires born√©s (ex: base 300-800ms + jitter) entre appels API
- Backoff adaptatif sur erreurs 429/5xx (ex: 1s, 2s, 4s, max 10s) avec limite de tentatives
- Respecter un budget de requ√™tes par fen√™tre (token bucket simple en m√©moire)
- Param√©trer via `parallel_config.py` (seuils, bornes, max backoff)
- Journaliser les pauses appliqu√©es (sans modifier les messages de logs existants)

**Validation**: Taux d'erreurs 429 en baisse significative sur un run de test; pas d'impact notable sur le d√©bit global (valider sur quickstart sc√©nario 5 et runs partiels).

## Nouvelles T√¢ches - Fonctionnalit√© Recherche par Domaine

### T032: Refactoring authentification API pour TrendTrack [P0]
**Type**: Core  
**Dependencies**: T021  
**Files**: `trendtrack-scraper-final/auth/api_auth_client.py`, `trendtrack-scraper-final/auth/dom_auth_client.py`  
**Description**: Remplacer l'authentification DOM par une authentification API pour TrendTrack.

**Impl√©mentation**:
- Cr√©er `api_auth_client.py` avec la requ√™te API de login fournie
- Cr√©er `dom_auth_client.py` pour maintenir la compatibilit√© (legacy)
- Impl√©menter la gestion des tokens/sessions API
- Ajouter la validation des r√©ponses d'authentification
- Maintenir la compatibilit√© avec l'existant

**Validation**: Authentification API fonctionne, compatibilit√© maintenue avec scraper traditionnel.

### T033: Architecture Playwright headless + stealth pour TrendTrack [P0]
**Type**: Core  
**Dependencies**: T032  
**Files**: `trendtrack-scraper-final/browser/stealth_browser.py`, `trendtrack-scraper-final/browser/legacy_browser.py`  
**Description**: Impl√©menter Playwright headless avec configuration stealth pour TrendTrack.

**Impl√©mentation**:
- Cr√©er `stealth_browser.py` avec configuration anti-d√©tection
- Ajouter user-agent rotation, viewport randomisation
- Impl√©menter la gestion des cookies et localStorage
- Cr√©er `legacy_browser.py` pour compatibilit√©
- Configuration via `trendtrack_config.py`

**Validation**: Navigation furtive fonctionne, pas de d√©tection anti-bot.

### T034: Syst√®me de pauses al√©atoires pour TrendTrack [P0]
**Type**: Core  
**Dependencies**: T033  
**Files**: `trendtrack-scraper-final/utils/anti_detection.py`, `trendtrack-scraper-final/config/stealth_config.py`  
**Description**: Impl√©menter un syst√®me de pauses al√©atoires pour √©viter la d√©tection.

**Impl√©mentation**:
- Cr√©er `anti_detection.py` avec pauses configurables
- Impl√©menter jitter al√©atoire (ex: 1-3s + random)
- Ajouter pauses entre requ√™tes et entre pages
- Configuration via `stealth_config.py`
- Logging des pauses appliqu√©es

**Validation**: Pauses appliqu√©es correctement, pas d'impact sur la performance.

### T035: Architecture de fichiers claire pour TrendTrack [P1]
**Type**: Architecture  
**Dependencies**: T032, T033, T034  
**Files**: `trendtrack-scraper-final/` (restructuration)  
**Description**: Restructurer l'architecture de fichiers pour s√©parer clairement les responsabilit√©s.

**Impl√©mentation**:
```
trendtrack-scraper-final/
‚îú‚îÄ‚îÄ auth/                    # Authentification
‚îÇ   ‚îú‚îÄ‚îÄ api_auth_client.py
‚îÇ   ‚îî‚îÄ‚îÄ dom_auth_client.py
‚îú‚îÄ‚îÄ browser/                 # Gestion navigateur
‚îÇ   ‚îú‚îÄ‚îÄ stealth_browser.py
‚îÇ   ‚îî‚îÄ‚îÄ legacy_browser.py
‚îú‚îÄ‚îÄ scrapers/               # Scrapers
‚îÇ   ‚îú‚îÄ‚îÄ traditional/        # Scraper traditionnel
‚îÇ   ‚îî‚îÄ‚îÄ domain_search/      # Nouvelle fonctionnalit√©
‚îú‚îÄ‚îÄ utils/                  # Utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ anti_detection.py
‚îÇ   ‚îî‚îÄ‚îÄ selectors.py
‚îú‚îÄ‚îÄ config/                 # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ stealth_config.py
‚îÇ   ‚îî‚îÄ‚îÄ trendtrack_config.py
‚îî‚îÄ‚îÄ legacy/                 # Code legacy (√† migrer)
```

**Validation**: Architecture claire, s√©paration des responsabilit√©s, pas de r√©gression.

### T036: Scraper recherche par domaine [P1]
**Type**: Core  
**Dependencies**: T035  
**Files**: `trendtrack-scraper-final/scrapers/domain_search/domain_scraper.py`  
**Description**: Impl√©menter le scraper de recherche par domaine sp√©cifique.

**Impl√©mentation**:
- Cr√©er `domain_scraper.py` pour recherche par domaine
- R√©utiliser les s√©lecteurs HTML existants
- Impl√©menter la logique de recherche cibl√©e (1 domaine √† la fois)
- Extraire la premi√®re ligne correspondante du tableau
- Int√©grer le syst√®me de pauses al√©atoires

**Validation**: Recherche par domaine fonctionne, donn√©es extraites correctement.

### T037: Int√©gration base de donn√©es pour recherche par domaine [P1]
**Type**: Integration  
**Dependencies**: T036  
**Files**: `trendtrack-scraper-final/database/domain_integration.py`  
**Description**: Int√©grer les r√©sultats de recherche par domaine dans la m√™me base que le scraper traditionnel.

**Impl√©mentation**:
- Cr√©er `domain_integration.py` pour l'int√©gration BDD
- R√©utiliser les tables `shops` et `analytics` existantes
- Impl√©menter la logique de d√©tection de doublons
- Ajouter le champ `search_method` (traditional/domain_search)
- Maintenir la coh√©rence des donn√©es

**Validation**: Donn√©es stock√©es correctement, pas de doublons, coh√©rence maintenue.

### T038: Tests de r√©gression pour nouvelle fonctionnalit√© [P1]
**Type**: Test  
**Dependencies**: T037  
**Files**: `trendtrack-scraper-final/tests/test_domain_search.py`  
**Description**: Cr√©er des tests pour la nouvelle fonctionnalit√© de recherche par domaine.

**Impl√©mentation**:
- Tester l'authentification API
- Tester la recherche par domaine
- Tester l'extraction des donn√©es
- Tester l'int√©gration base de donn√©es
- Tester le syst√®me anti-d√©tection

**Validation**: Tous les tests passent, couverture > 90%.

### T039: Migration scraper traditionnel vers nouvelle architecture [P2]
**Type**: Migration  
**Dependencies**: T038  
**Files**: `trendtrack-scraper-final/scrapers/traditional/`  
**Description**: Migrer le scraper traditionnel vers la nouvelle architecture.

**Impl√©mentation**:
- Migrer `smart_scraper_intelligent.py` vers `traditional/`
- Adapter pour utiliser la nouvelle authentification API
- Int√©grer le syst√®me de pauses al√©atoires
- Maintenir la compatibilit√© avec l'existant
- Tests de r√©gression

**Validation**: Scraper traditionnel fonctionne avec nouvelle architecture.

### T040: Documentation et d√©ploiement [P2]
**Type**: Documentation  
**Dependencies**: T039  
**Files**: `trendtrack-scraper-final/docs/`, `trendtrack-scraper-final/README.md`  
**Description**: Documenter la nouvelle fonctionnalit√© et pr√©parer le d√©ploiement.

**Impl√©mentation**:
- Documenter l'architecture de fichiers
- Cr√©er le guide d'utilisation
- Documenter les nouvelles configurations
- Pr√©parer les scripts de d√©ploiement
- Mettre √† jour le README

**Validation**: Documentation compl√®te, d√©ploiement pr√™t.

## T√¢ches de Gouvernance & Validation

### T029: Proposition d‚Äôalignement sch√©ma Prod/Test (validation avant migration)
**Type**: Gouvernance/Validation  
**Dependencies**: T002  
**Files**: `specs/001-name-trendtrack-scraper/plan/db-schema-compare.md`, `specs/001-name-trendtrack-scraper/plan/data-model.md`  
**Description**: Pr√©parer une proposition d‚Äôalignement des sch√©mas Prod/Test, soumise √† validation avant toute migration.

**Impl√©mentation**:
- Synth√©tiser les √©carts (types, colonnes, index) √† partir de `db-schema-compare.md`
- Proposer un sch√©ma cible unique (types stricts, index minimaux, champs retenus)
- Documenter les impacts potentiels (lecture/√©criture API, scripts, performances)
- D√©finir un plan de migration s√©curis√© (√©tapes, rollback, validations)

**Validation**: Validation explicite de l‚Äôutilisateur sur la proposition (OK/KO) avant ex√©cution de toute migration.

## Polish Tasks

### T024: Unit Tests for All Services [P]
**Type**: Polish  
**Dependencies**: T010-T016  
**Files**: `tests/unit/test_*.py`  
**Description**: Cr√©er des tests unitaires complets pour tous les services.

**Impl√©mentation**:
- Cr√©er des tests unitaires pour WebsiteService, MetricsService, SessionService
- Cr√©er des tests unitaires pour ScrapingService, WorkerService, ErrorHandlingService
- Cr√©er des tests unitaires pour MetricValidationService
- Ajouter le reporting de couverture de tests

**Validation**: Tous les tests unitaires passent, couverture > 90%.

### T025: Performance Optimization [P]
**Type**: Polish  
**Dependencies**: T014, T021  
**Files**: `services/performance_service.py`  
**Description**: Optimiser les performances pour le traitement parall√®le et les appels API.

**Impl√©mentation**:
- Optimiser les requ√™tes de base de donn√©es et l'indexation
- Impl√©menter le pool de connexions et la mise en cache
- Optimiser les performances des workers parall√®les
- Ajouter le monitoring de performance et les m√©triques

**Validation**: Les performances r√©pondent aux exigences (1-2 min/site web, 95%+ succ√®s API).

### T026: Documentation and API Documentation [P]
**Type**: Polish  
**Dependencies**: T017-T020  
**Files**: `docs/`, `README.md`  
**Description**: Cr√©er une documentation compl√®te et une documentation API.

**Impl√©mentation**:
- Cr√©er la documentation API avec des exemples
- Cr√©er le guide utilisateur et le guide de d√©pannage
- Cr√©er la documentation d√©veloppeur
- Mettre √† jour le README avec les instructions de configuration et d'utilisation

**Validation**: La documentation est compl√®te et pr√©cise.

### T027: End-to-End Validation [P]
**Type**: Polish  
**Dependencies**: T006-T009  
**Files**: `tests/e2e/test_full_scraping_flow.py`  
**Description**: Cr√©er des tests de validation end-to-end pour le flux de scraping complet.

**Impl√©mentation**:
- Cr√©er un test E2E pour le workflow de scraping complet
- Tester les 5 sc√©narios de quickstart.md
- Valider les exigences de performance
- Tester les sc√©narios d'erreur et la r√©cup√©ration

**Validation**: Tous les tests E2E passent, le syst√®me fonctionne end-to-end.

### T028: Production Deployment Preparation
**Type**: Polish  
**Dependencies**: T021-T023  
**Files**: `deployment/`, `docker/`  
**Description**: Pr√©parer le syst√®me pour le d√©ploiement en production sur VPS.

**Impl√©mentation**:
- Cr√©er les scripts de d√©ploiement et la configuration
- Configurer les variables d'environnement de production
- Cr√©er les proc√©dures de sauvegarde et r√©cup√©ration
- Configurer le monitoring et l'alerte

**Validation**: Le syst√®me est pr√™t pour le d√©ploiement en production.

## Parallel Execution Examples

### Phase 1: Configuration et Tests (Parall√®le)
```bash
# Ex√©cuter les t√¢ches de configuration
T√¢che T001: Configuration de l'environnement et d√©pendances
T√¢che T002: Configuration du sch√©ma de base de donn√©es [P]

# Ex√©cuter les tests de contrats en parall√®le
T√¢che T003: Tests de contrats de l'entit√© Website [P]
T√¢che T004: Tests de contrats de l'entit√© Metrics [P]
T√¢che T005: Tests de contrats de l'entit√© ScrapingSession [P]

# Ex√©cuter les tests d'int√©gration en parall√®le
T√¢che T006: Test d'int√©gration - Scraping de base de site web [P]
T√¢che T007: Test d'int√©gration - Traitement parall√®le de workers [P]
T√¢che T008: Test d'int√©gration - Gestion d'erreurs et r√©cup√©ration [P]
T√¢che T009: Test d'int√©gration - Validation des m√©triques [P]
```

### Phase 2: Impl√©mentation Core (S√©quentielle)
```bash
# Entit√©s core (s√©quentiel √† cause des d√©pendances)
T√¢che T010: Impl√©mentation de l'entit√© Website
T√¢che T011: Impl√©mentation de l'entit√© Metrics
T√¢che T012: Impl√©mentation de l'entit√© ScrapingSession

# Services core (s√©quentiel √† cause des d√©pendances)
T√¢che T013: Impl√©mentation du service de scraping de base
T√¢che T014: Impl√©mentation du service de workers parall√®les
T√¢che T015: Impl√©mentation du service de gestion d'erreurs
T√¢che T016: Impl√©mentation du service de validation des m√©triques
```

### Phase 3: Impl√©mentation API (Parall√®le)
```bash
# Endpoints API (parall√®le - fichiers diff√©rents)
T√¢che T017: Impl√©mentation des endpoints API Website [P]
T√¢che T018: Impl√©mentation des endpoints API Metrics [P]
T√¢che T019: Impl√©mentation des endpoints API ScrapingSession [P]
T√¢che T020: Impl√©mentation de l'endpoint API de v√©rification de sant√© [P]
```

### Phase 4: Int√©gration (S√©quentielle)
```bash
# Services d'int√©gration (s√©quentiel √† cause des d√©pendances)
T√¢che T021: Int√©gration du client API MyToolsPlan
T√¢che T022: Service d'int√©gration de base de donn√©es
T√¢che T023: Int√©gration du logging et monitoring
```

### Phase 5: Finition (Parall√®le)
```bash
# T√¢ches de finition (parall√®le - fichiers diff√©rents)
T√¢che T024: Tests unitaires pour tous les services [P]
T√¢che T025: Optimisation des performances [P]
T√¢che T026: Documentation et documentation API [P]
T√¢che T027: Validation end-to-end [P]
T√¢che T028: Pr√©paration du d√©ploiement en production
```

## T√¢ches de Correction (P0 - Critique)

### T041: Impl√©menter la m√©trique visits manquante
**Type**: Correction  
**Dependencies**: T014 (Correction avg_visit_duration)  
**Files**: `production_scraper_parallel.py`, `launch_workers_by_status.py`  
**Description**: La m√©trique `visits` affiche "Aucune tentative" car elle n'est pas impl√©ment√©e dans le scraper.  
**Probl√®me**: M√©trique importante manquante dans les statistiques de scraping.  
**Solution**: 
- Analyser o√π la m√©trique `visits` devrait √™tre r√©cup√©r√©e (API ou DOM scraping)
- Impl√©menter la logique de r√©cup√©ration
- Ajouter la m√©trique dans `format_analytics_for_api()`
- Tester et valider le fonctionnement

**Acceptance Criteria**:
- [ ] La m√©trique `visits` n'affiche plus "Aucune tentative"
- [ ] Les donn√©es de `visits` sont correctement r√©cup√©r√©es et stock√©es
- [ ] Les statistiques de comptage incluent `visits`
- [ ] Tests de validation passent

## Nouvelles T√¢ches - Migration Base de Donn√©es

### T046: Migration des types de donn√©es [P0]
**Type**: Core  
**Dependencies**: T045  
**Files**: `trendtrack-scraper-final/data/migration_*.sql`  
**Description**: Corriger les 7 types de donn√©es incorrects identifi√©s dans l'analyse.
**Impl√©mentation**:
```sql
-- Table shops
ALTER TABLE shops ALTER COLUMN monthly_visits TYPE INTEGER;
ALTER TABLE shops ALTER COLUMN live_ads TYPE TEXT;
ALTER TABLE shops ALTER COLUMN page_number TYPE TEXT;
ALTER TABLE shops ALTER COLUMN scraped_at TYPE TEXT;
ALTER TABLE shops ALTER COLUMN updated_at TYPE DATE;
ALTER TABLE shops ALTER COLUMN creation_date TYPE DATE;
ALTER TABLE shops ALTER COLUMN year_founded TYPE DATE;

-- Table analytics
ALTER TABLE analytics ALTER COLUMN organic_traffic TYPE INTEGER;
ALTER TABLE analytics ALTER COLUMN bounce_rate TYPE NUMERIC;
ALTER TABLE analytics ALTER COLUMN branded_traffic TYPE INTEGER;
ALTER TABLE analytics ALTER COLUMN visits TYPE INTEGER;
ALTER TABLE analytics ALTER COLUMN traffic TYPE INTEGER;
ALTER TABLE analytics ALTER COLUMN paid_search_traffic TYPE INTEGER;
ALTER TABLE analytics ALTER COLUMN percent_branded_traffic TYPE NUMERIC;
ALTER TABLE analytics ALTER COLUMN updated_at TYPE DATE;
```

### T047: Ajout des champs manquants [P1]
**Type**: Core  
**Dependencies**: T046  
**Files**: `trendtrack-scraper-final/data/migration_*.sql`  
**Description**: Ajouter les 11 champs manquants identifi√©s dans l'analyse.
**Impl√©mentation**:
```sql
-- Nouveaux champs shops (10 champs)
ALTER TABLE shops ADD COLUMN total_products INTEGER;
ALTER TABLE shops ADD COLUMN pixel_google INTEGER;
ALTER TABLE shops ADD COLUMN pixel_facebook INTEGER;
ALTER TABLE shops ADD COLUMN aov NUMERIC;
ALTER TABLE shops ADD COLUMN market_us NUMERIC;
ALTER TABLE shops ADD COLUMN market_uk NUMERIC;
ALTER TABLE shops ADD COLUMN market_de NUMERIC;
ALTER TABLE shops ADD COLUMN market_ca NUMERIC;
ALTER TABLE shops ADD COLUMN market_au NUMERIC;
ALTER TABLE shops ADD COLUMN market_fr NUMERIC;

-- Nouveau champ analytics (1 champ)
ALTER TABLE analytics ADD COLUMN cpc NUMERIC;
```

### T048: Mise √† jour du code de scraping [P1]
**Type**: Core  
**Dependencies**: T047  
**Files**: `trendtrack-scraper-final/scrapers/`, `trendtrack-scraper-final/src/`  
**Description**: Adapter tous les scrapers pour les nouveaux types et champs.
**Impl√©mentation**:
- Modifier les scrapers JavaScript (TrendTrack)
- Modifier les scrapers Python (MyToolsPlan, Domain Search)
- Adapter les types de donn√©es dans le code
- Ajouter la logique pour alimenter les nouveaux champs
- Mettre √† jour les requ√™tes SQL

### T049: Script de migration de base de donn√©es [P0]
**Type**: Core  
**Dependencies**: T048  
**Files**: `trendtrack-scraper-final/data/migrate_to_final_structure.py`  
**Description**: Cr√©er un script complet de migration de la structure actuelle vers la finale.
**Impl√©mentation**:
- Script Python avec sauvegarde automatique
- Migration des types de donn√©es
- Ajout des nouveaux champs
- Validation de l'int√©grit√© des donn√©es
- Rollback en cas d'erreur
- Tests de validation post-migration

### T050: Validation des donn√©es apr√®s migration [P1]
**Type**: Test  
**Dependencies**: T049  
**Files**: `trendtrack-scraper-final/test_migration_validation.py`  
**Description**: V√©rifier que tous les champs sont correctement aliment√©s apr√®s migration.
**Impl√©mentation**:
- Tests de validation des types
- Tests de validation des nouveaux champs
- Tests de performance post-migration
- Tests d'int√©grit√© des donn√©es
- Comparaison avant/apr√®s migration

### T073: V√©rification format ISO 8601 UTC pour toutes les dates en BDD [P0]
**Type**: Correction  
**Dependencies**: T049  
**Files**: `trendtrack-scraper-final/`, `sem-scraper-final/`  
**Description**: V√©rifier que toutes les dates enregistr√©es en base de donn√©es sont au format ISO 8601 UTC timestamp.
**Probl√®me**: Incoh√©rence dans les formats de dates entre les scrapers TrendTrack et SEM.
**Solution**: 
- Auditer tous les enregistrements de dates dans le code
- V√©rifier les formats utilis√©s (datetime.now(), CURRENT_TIMESTAMP, etc.)
- Standardiser sur le format ISO 8601 UTC (ex: "2025-09-18T10:30:45.123Z")
- Corriger tous les scrapers (TrendTrack JavaScript + SEM Python)
- Tests de validation des formats de dates

**Acceptance Criteria**:
- [ ] Toutes les dates en BDD sont au format ISO 8601 UTC
- [ ] Le code TrendTrack utilise le format ISO 8601 UTC
- [ ] Le code SEM utilise le format ISO 8601 UTC
- [ ] Tests de validation des formats de dates passent
- [ ] Documentation des formats de dates mise √† jour

### T074: Audit du fonctionnement de l'API endpoint test [P0]
**Type**: Audit  
**Dependencies**: Aucune  
**Files**: `http://37.59.102.7:8001/test/shops/with-analytics-ordered?since=2025-07-10T00:00:00Z`  
**Description**: Auditer le fonctionnement de l'API endpoint de test pour valider la qualit√© des donn√©es et la performance.
**Objectif**: S'assurer que l'API de test fonctionne correctement et fournit des donn√©es coh√©rentes pour l'environnement de test.

**Impl√©mentation**:
- Tester l'endpoint API avec diff√©rents param√®tres de filtrage
- Valider la structure des donn√©es retourn√©es (180 boutiques avec m√©triques compl√®tes)
- V√©rifier la coh√©rence des m√©triques (organic_traffic, paid_search_traffic, visits, etc.)
- Analyser les performances de l'API (temps de r√©ponse, stabilit√©)
- Tester les cas limites (dates invalides, param√®tres manquants)
- Valider l'environnement de test (database: "trendtrack_test.db")

**Validation**:
- [ ] L'API r√©pond correctement avec un statut 200
- [ ] Les 180 boutiques sont retourn√©es avec toutes les m√©triques
- [ ] La structure JSON est coh√©rente et valide
- [ ] Les m√©triques num√©riques sont dans les plages attendues
- [ ] L'environnement de test est correctement identifi√©
- [ ] Les performances sont acceptables (< 2 secondes de r√©ponse)
- [ ] La gestion d'erreurs fonctionne pour les cas limites

**Donn√©es de r√©f√©rence** (bas√©es sur l'audit initial):
- **URL**: http://37.59.102.7:8001/test/shops/with-analytics-ordered?since=2025-07-10T00:00:00Z
- **Environnement**: TEST
- **Base de donn√©es**: trendtrack_test.db
- **Nombre de boutiques**: 180
- **M√©triques disponibles**: monthly_visits, year_founded, total_products, aov, pixel_google, pixel_facebook, organic_traffic, bounce_rate, avg_visit_duration, visits, traffic, branded_traffic, percent_branded_traffic, paid_search_traffic, cpc, conversion_rate, market_* (us, uk, de, ca, au, fr)

## Task Dependencies Summary

**Chemin Critique**: T074 ‚Üí T001 ‚Üí T002 ‚Üí T032 ‚Üí T033 ‚Üí T034 ‚Üí T035 ‚Üí T036 ‚Üí T037 ‚Üí T038 ‚Üí T039 ‚Üí T040 ‚Üí T046 ‚Üí T047 ‚Üí T048 ‚Üí T049 ‚Üí T050

**Opportunit√©s Parall√®les**: 
- Audit API (T074) peut s'ex√©cuter en parall√®le avec les autres t√¢ches P0
- Tests de contrats (T003-T009) peuvent s'ex√©cuter en parall√®le apr√®s T002
- Endpoints API (T017-T020) peuvent s'ex√©cuter en parall√®le apr√®s les entit√©s core
- T√¢ches de finition (T024-T027) peuvent s'ex√©cuter en parall√®le apr√®s l'int√©gration
- Nouvelles t√¢ches TrendTrack (T032-T034) peuvent s'ex√©cuter en parall√®le apr√®s T021
- T√¢ches de correction (T041) peuvent s'ex√©cuter en parall√®le avec les autres P0
- T√¢ches de migration (T046-T050) peuvent s'ex√©cuter en parall√®le apr√®s T045

**Timeline Estim√©**: 
- Configuration et Tests: 2-3 jours
- Impl√©mentation Core: 5-7 jours  
- Impl√©mentation API: 2-3 jours
- Int√©gration: 3-4 jours
- **Nouvelle fonctionnalit√© TrendTrack**: 4-6 jours
- **Migration base de donn√©es**: 3-4 jours
- Finition: 2-3 jours
- **Total**: 21-30 jours

---

*T√¢ches g√©n√©r√©es: 2025-09-16*
*Pr√™t pour l'impl√©mentation suivant les principes constitutionnels*

# Implementation Tasks: Scraper SEM Parall√®le

**Feature**: Scraper SEM Parall√®le  
**Branch**: `001-name-trendtrack-scraper`  
**Date**: 2025-09-16  
**Generated from**: plan.md, data-model.md, research.md, quickstart.md, contracts/openapi.yaml

## Task Overview

**Total Tasks**: 28  
**Parallel Tasks**: 15 (marked with [P])  
**Sequential Tasks**: 13  

**Strat√©gie d'Ex√©cution**: Approche TDD avec tests avant impl√©mentation, ex√©cution ordonn√©e par d√©pendances, ex√©cution parall√®le quand possible.

## Priorisation (P0/P1/P2)

- P0 (Critique, √† ex√©cuter en premier):
  - T001, T002 (Audit + comparaison BDD)
  - T006, T007, T008, T009 (Tests de r√©gression/baseline)
  - T016 (Service de validation des m√©triques)
  - T015 (Service de gestion d'erreurs)
  - T021 (Int√©gration client API MyToolsPlan)

- P1 (Important, apr√®s P0):
  - T010, T011, T012 (Entit√©s et services de base si √©carts d√©tect√©s)
  - T013, T014 (Services scraping de base + workers parall√®les)
  - T017, T018, T019, T020 (Endpoints API)

- P2 (Finition/optimisation):
  - T024, T025, T026, T027, T028 (Unitaires, perf, docs, E2E, d√©ploiement)

## Phase 0: Audit du Projet Existant (REMPLACE le Setup)

### T001: Audit rapide du code et de la doc existants [P]
**Type**: Audit  
**Dependencies**: None  
**Files**: `sem-scraper-final/production_scraper_parallel.py`, `trendtrack-scraper-final/`, `ubuntu/README/*`, `.specify/memory/constitution.md`  
**Description**: Cartographier les composants d√©j√† en place (scraper, APIs, DB, workers, logs) et v√©rifier leur coh√©rence avec la constitution `.specify`.

**Implementation**:
- Lister les points d‚Äôentr√©e (scripts de lancement, workers, endpoints)
- V√©rifier la pr√©sence du syst√®me de rollback et l‚Äô√©tat des logs
- Valider que la logique de validation m√©triques est en place et utilis√©e
- Noter les divergences doc ‚Üî code

**Validation**: Rapport d‚Äôaudit bref (10 lignes) + liste de divergences, valid√©s par l‚Äôutilisateur.

### T002: V√©rification sch√©mas BDD prod/test (sans migration) [P]
**Type**: Audit  
**Dependencies**: T001  
**Files**: `trendtrack-final-scraper/data/trendtrack.db`, `trendtrack-final-scraper/data/trendtrack_test.db`  
**Description**: Comparer les sch√©mas des deux BDD, sans cr√©er de nouvelles migrations.

**Implementation**:
- Exporter la structure des tables (shops, analytics, scraping_sessions, workers)
- Comparer colonnes, types, index
- Lister les √©carts √©ventuels

**Validation**: Rapport de comparaison valid√© par l‚Äôutilisateur; aucune modification appliqu√©e.

## Test Tasks (R√©gression sur existant)

### T003: Website Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_website_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© Website bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD Website (GET, POST, PUT, DELETE /websites)
- Valider que les sch√©mas request/response correspondent √† la sp√©cification OpenAPI
- Tester le filtrage par statut et la pagination
- Tester la gestion d'erreurs (404, 400, 409)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T004: Metrics Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_metrics_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© Metrics bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD metrics (GET, PUT /websites/{id}/metrics)
- Valider le sch√©ma des 8 m√©triques (organic_traffic, paid_search_traffic, visits, bounce_rate, avg_visit_duration, branded_traffic, conversion_rate, percent_branded_traffic)
- Tester les r√®gles de validation des m√©triques
- Tester la gestion d'erreurs (404, 400)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T005: ScrapingSession Entity Contract Tests [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/contracts/test_session_contracts.py`  
**Description**: Cr√©er des tests de contrats pour l'entit√© ScrapingSession bas√©s sur le sch√©ma OpenAPI.

**Impl√©mentation**:
- Tester les op√©rations CRUD session (GET, POST, DELETE /scraping/sessions)
- Tester l'endpoint de statut de session (/scraping/sessions/{id}/status)
- Valider le sch√©ma de session et les transitions de statut
- Tester la gestion d'erreurs (404, 400)

**Validation**: Tests ciblent l‚ÄôAPI/impl√©mentation actuelle; servent de filet de r√©gression.

### T006: Integration Test - Basic Website Scraping [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_basic_scraping.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de scraping de base d'un site web depuis quickstart.md.

**Impl√©mentation**:
- Tester le scraping end-to-end d'un site web unique
- Valider la collecte de m√©triques et les mises √† jour de statut
- Tester la cr√©ation de session et le monitoring
- Valider les r√©ponses des endpoints API

**Validation**: Doit passer sur le code actuel (sert de baseline), sinon ouvrir bug.

### T007: Integration Test - Parallel Worker Processing [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_parallel_workers.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de traitement parall√®le de workers depuis quickstart.md.

**Impl√©mentation**:
- Tester le traitement parall√®le de plusieurs sites web
- Valider la distribution de charge des workers
- Tester le monitoring du progr√®s de session
- Valider le traitement concurrent

**Validation**: Doit passer sur le code actuel (baseline), sinon ouvrir bug.

### T008: Integration Test - Error Handling and Recovery [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_error_handling.py`  
**Description**: Cr√©er un test d'int√©gration pour le sc√©nario de gestion d'erreurs et de d√©gradation gracieuse depuis quickstart.md.

**Impl√©mentation**:
- Tester la gestion des sites web invalides
- Valider la d√©gradation gracieuse avec des √©checs partiels
- Tester le logging d'erreurs et les mises √† jour de statut
- Valider la completion de session avec des r√©sultats mixtes

**Validation**: Doit passer (baseline), sinon ouvrir bug.

### T009: Integration Test - Metric Validation [P]
**Type**: Test  
**Dependencies**: T002  
**Files**: `tests/integration/test_metric_validation.py`  
**Description**: Cr√©er un test d'int√©gration pour la logique de validation des m√©triques depuis quickstart.md.

**Impl√©mentation**:
- Tester la collecte des 8 m√©triques
- Valider la logique de statut (completed vs partial)
- Tester les m√©triques calcul√©es (percent_branded_traffic)
- Valider les r√®gles de validation des m√©triques

**Validation**: Doit passer (baseline), sinon ouvrir bug.

## Core Implementation Tasks

### T010: Website Entity Implementation
**Type**: Core  
**Dependencies**: T003  
**Files**: `models/website.py`, `services/website_service.py`  
**Description**: Impl√©menter l'entit√© Website et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le Website avec tous les champs de data-model.md
- Impl√©menter WebsiteService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour shop_url, scraping_status, timestamps
- Impl√©menter la logique de transition de statut

**Validation**: Les tests de contrats T003 passent, l'entit√© Website fonctionne correctement.

### T011: Metrics Entity Implementation
**Type**: Core  
**Dependencies**: T004  
**Files**: `models/metrics.py`, `services/metrics_service.py`  
**Description**: Impl√©menter l'entit√© Metrics et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le Metrics avec les 8 champs de m√©triques
- Impl√©menter MetricsService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour les champs num√©riques et les plages d√©cimales
- Impl√©menter la logique de champ calcul√© (percent_branded_traffic)

**Validation**: Les tests de contrats T004 passent, l'entit√© Metrics fonctionne correctement.

### T012: ScrapingSession Entity Implementation
**Type**: Core  
**Dependencies**: T005  
**Files**: `models/scraping_session.py`, `services/session_service.py`  
**Description**: Impl√©menter l'entit√© ScrapingSession et le service bas√©s sur le mod√®le de donn√©es.

**Impl√©mentation**:
- Cr√©er le mod√®le ScrapingSession avec tous les champs
- Impl√©menter SessionService avec les op√©rations CRUD
- Ajouter les r√®gles de validation pour worker_count, websites_per_worker
- Impl√©menter la gestion du statut de session

**Validation**: Les tests de contrats T005 passent, l'entit√© ScrapingSession fonctionne correctement.

### T013: Basic Scraping Service Implementation
**Type**: Core  
**Dependencies**: T006, T010, T011  
**Files**: `services/scraping_service.py`  
**Description**: Impl√©menter le service de scraping de base pour le traitement d'un site web unique.

**Impl√©mentation**:
- Cr√©er ScrapingService avec traitement d'un site web unique
- Impl√©menter l'authentification MyToolsPlan et la gestion de session
- Ajouter la collecte de m√©triques de base (APIs + DOM scraping)
- Impl√©menter la logique de mise √† jour de statut

**Validation**: Le test d'int√©gration T006 passe, le scraping de base fonctionne.

### T014: Parallel Worker Service Implementation
**Type**: Core  
**Dependencies**: T007, T012, T013  
**Files**: `services/worker_service.py`, `workers/parallel_worker.py`  
**Description**: Impl√©menter le service de workers parall√®les pour le traitement concurrent.

**Impl√©mentation**:
- Cr√©er WorkerService avec la logique de traitement parall√®le
- Impl√©menter la classe ParallelWorker avec asyncio
- Ajouter la gestion du pool de workers et la distribution de charge
- Impl√©menter le suivi du progr√®s de session

**Validation**: Le test d'int√©gration T007 passe, le traitement parall√®le fonctionne.

### T015: Error Handling Service Implementation
**Type**: Core  
**Dependencies**: T008, T013  
**Files**: `services/error_handling_service.py`  
**Description**: Impl√©menter une gestion d'erreurs compl√®te et une d√©gradation gracieuse.

**Impl√©mentation**:
- Cr√©er ErrorHandlingService avec la logique de retry
- Impl√©menter la d√©gradation gracieuse pour les √©checs partiels
- Ajouter un logging d'erreurs complet
- Impl√©menter les m√©canismes de r√©cup√©ration d'erreurs

**Validation**: Le test d'int√©gration T008 passe, la gestion d'erreurs fonctionne correctement.

### T016: Metric Validation Service Implementation
**Type**: Core  
**Dependencies**: T009, T011  
**Files**: `services/metric_validation_service.py`  
**Description**: Impl√©menter la logique de validation adaptative des m√©triques.

**Impl√©mentation**:
- Cr√©er MetricValidationService avec validation dynamique
- Impl√©menter la logique de validation des 8 m√©triques
- Ajouter la d√©termination de statut (completed/partial/failed/na)
- Impl√©menter la validation des m√©triques calcul√©es

**Validation**: Le test d'int√©gration T009 passe, la validation des m√©triques fonctionne correctement.

## API Implementation Tasks

### T017: Website API Endpoints Implementation
**Type**: API  
**Dependencies**: T010  
**Files**: `api/website_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© Website.

**Impl√©mentation**:
- Impl√©menter GET /websites (liste avec filtrage)
- Impl√©menter POST /websites (cr√©ation)
- Impl√©menter GET /websites/{id} (obtenir d√©tails)
- Impl√©menter PUT /websites/{id} (mise √† jour)
- Impl√©menter DELETE /websites/{id} (suppression)

**Validation**: Tous les endpoints API Website fonctionnent correctement, correspondent au contrat OpenAPI.

### T018: Metrics API Endpoints Implementation
**Type**: API  
**Dependencies**: T011  
**Files**: `api/metrics_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© Metrics.

**Impl√©mentation**:
- Impl√©menter GET /websites/{id}/metrics (obtenir m√©triques)
- Impl√©menter PUT /websites/{id}/metrics (mettre √† jour m√©triques)
- Ajouter une gestion d'erreurs et validation appropri√©es
- S'assurer que les sch√©mas de r√©ponse correspondent √† OpenAPI

**Validation**: Tous les endpoints API Metrics fonctionnent correctement, correspondent au contrat OpenAPI.

### T019: ScrapingSession API Endpoints Implementation
**Type**: API  
**Dependencies**: T012  
**Files**: `api/session_endpoints.py`  
**Description**: Impl√©menter les endpoints FastAPI pour l'entit√© ScrapingSession.

**Impl√©mentation**:
- Impl√©menter GET /scraping/sessions (lister sessions)
- Impl√©menter POST /scraping/sessions (d√©marrer session)
- Impl√©menter GET /scraping/sessions/{id} (obtenir d√©tails)
- Impl√©menter DELETE /scraping/sessions/{id} (annuler session)
- Impl√©menter GET /scraping/sessions/{id}/status (obtenir statut)

**Validation**: Tous les endpoints API ScrapingSession fonctionnent correctement, correspondent au contrat OpenAPI.

### T020: Health Check API Endpoint Implementation
**Type**: API  
**Dependencies**: None  
**Files**: `api/health_endpoints.py`  
**Description**: Impl√©menter l'endpoint de v√©rification de sant√©.

**Impl√©mentation**:
- Impl√©menter l'endpoint GET /health
- Ajouter la v√©rification de connectivit√© base de donn√©es
- Ajouter la validation du statut syst√®me
- Retourner le statut de sant√© appropri√©

**Validation**: L'endpoint de v√©rification de sant√© retourne le statut correct.

## Integration Tasks

### T021: MyToolsPlan API Client Integration
**Type**: Integration  
**Dependencies**: T013  
**Files**: `clients/mytoolsplan_client.py`  
**Description**: Impl√©menter le client API MyToolsPlan avec authentification et gestion de session.

**Impl√©mentation**:
- Cr√©er MyToolsPlanClient avec authentification
- Impl√©menter la synchronisation de cookies entre domaines
- Ajouter les endpoints API pour organic.Summary, organic.OverviewTrend, engagement
- Impl√©menter le syst√®me de fallback (sam2.mytoolsplan.xyz)

**Validation**: Les APIs MyToolsPlan s'int√®grent correctement, l'authentification fonctionne.

### T022: Database Integration Service
**Type**: Integration  
**Dependencies**: T010, T011, T012  
**Files**: `services/database_service.py`  
**Description**: Impl√©menter l'int√©gration base de donn√©es avec support de double base de donn√©es.

**Impl√©mentation**:
- Cr√©er DatabaseService avec basculement base de donn√©es production/test
- Impl√©menter le pool de connexions et la gestion de transactions
- Ajouter le support de migration de base de donn√©es
- Impl√©menter les proc√©dures de sauvegarde et r√©cup√©ration

**Validation**: Les op√©rations de base de donn√©es fonctionnent correctement sur les deux bases.

### T023: Logging and Monitoring Integration
**Type**: Integration  
**Dependencies**: T015  
**Files**: `services/logging_service.py`, `monitoring/metrics_collector.py`  
**Description**: Impl√©menter un logging et monitoring complets.

**Impl√©mentation**:
- Cr√©er LoggingService avec logging structur√©
- Impl√©menter le monitoring pour les performances de scraping
- Ajouter le suivi d'erreurs et l'alerte
- Impl√©menter les outils d'analyse de logs et de d√©bogage

**Validation**: Le logging et monitoring fonctionnent correctement, fournissent des insights utiles.

## Polish Tasks

### T024: Unit Tests for All Services [P]
**Type**: Polish  
**Dependencies**: T010-T016  
**Files**: `tests/unit/test_*.py`  
**Description**: Cr√©er des tests unitaires complets pour tous les services.

**Impl√©mentation**:
- Cr√©er des tests unitaires pour WebsiteService, MetricsService, SessionService
- Cr√©er des tests unitaires pour ScrapingService, WorkerService, ErrorHandlingService
- Cr√©er des tests unitaires pour MetricValidationService
- Ajouter le reporting de couverture de tests

**Validation**: Tous les tests unitaires passent, couverture > 90%.

### T025: Performance Optimization [P]
**Type**: Polish  
**Dependencies**: T014, T021  
**Files**: `services/performance_service.py`  
**Description**: Optimiser les performances pour le traitement parall√®le et les appels API.

**Impl√©mentation**:
- Optimiser les requ√™tes de base de donn√©es et l'indexation
- Impl√©menter le pool de connexions et la mise en cache
- Optimiser les performances des workers parall√®les
- Ajouter le monitoring de performance et les m√©triques

**Validation**: Les performances r√©pondent aux exigences (1-2 min/site web, 95%+ succ√®s API).

### T026: Documentation and API Documentation [P]
**Type**: Polish  
**Dependencies**: T017-T020  
**Files**: `docs/`, `README.md`  
**Description**: Cr√©er une documentation compl√®te et une documentation API.

**Impl√©mentation**:
- Cr√©er la documentation API avec des exemples
- Cr√©er le guide utilisateur et le guide de d√©pannage
- Cr√©er la documentation d√©veloppeur
- Mettre √† jour le README avec les instructions de configuration et d'utilisation

**Validation**: La documentation est compl√®te et pr√©cise.

### T027: End-to-End Validation [P]
**Type**: Polish  
**Dependencies**: T006-T009  
**Files**: `tests/e2e/test_full_scraping_flow.py`  
**Description**: Cr√©er des tests de validation end-to-end pour le flux de scraping complet.

**Impl√©mentation**:
- Cr√©er un test E2E pour le workflow de scraping complet
- Tester les 5 sc√©narios de quickstart.md
- Valider les exigences de performance
- Tester les sc√©narios d'erreur et la r√©cup√©ration

**Validation**: Tous les tests E2E passent, le syst√®me fonctionne end-to-end.

### T028: Production Deployment Preparation
**Type**: Polish  
**Dependencies**: T021-T023  
**Files**: `deployment/`, `docker/`  
**Description**: Pr√©parer le syst√®me pour le d√©ploiement en production sur VPS.

**Impl√©mentation**:
- Cr√©er les scripts de d√©ploiement et la configuration
- Configurer les variables d'environnement de production
- Cr√©er les proc√©dures de sauvegarde et r√©cup√©ration
- Configurer le monitoring et l'alerte

**Validation**: Le syst√®me est pr√™t pour le d√©ploiement en production.

## Parallel Execution Examples

### Phase 1: Configuration et Tests (Parall√®le)
```bash
# Ex√©cuter les t√¢ches de configuration
T√¢che T001: Configuration de l'environnement et d√©pendances
T√¢che T002: Configuration du sch√©ma de base de donn√©es [P]

# Ex√©cuter les tests de contrats en parall√®le
T√¢che T003: Tests de contrats de l'entit√© Website [P]
T√¢che T004: Tests de contrats de l'entit√© Metrics [P]
T√¢che T005: Tests de contrats de l'entit√© ScrapingSession [P]

# Ex√©cuter les tests d'int√©gration en parall√®le
T√¢che T006: Test d'int√©gration - Scraping de base de site web [P]
T√¢che T007: Test d'int√©gration - Traitement parall√®le de workers [P]
T√¢che T008: Test d'int√©gration - Gestion d'erreurs et r√©cup√©ration [P]
T√¢che T009: Test d'int√©gration - Validation des m√©triques [P]
```

### Phase 2: Impl√©mentation Core (S√©quentielle)
```bash
# Entit√©s core (s√©quentiel √† cause des d√©pendances)
T√¢che T010: Impl√©mentation de l'entit√© Website
T√¢che T011: Impl√©mentation de l'entit√© Metrics
T√¢che T012: Impl√©mentation de l'entit√© ScrapingSession

# Services core (s√©quentiel √† cause des d√©pendances)
T√¢che T013: Impl√©mentation du service de scraping de base
T√¢che T014: Impl√©mentation du service de workers parall√®les
T√¢che T015: Impl√©mentation du service de gestion d'erreurs
T√¢che T016: Impl√©mentation du service de validation des m√©triques
```

### Phase 3: Impl√©mentation API (Parall√®le)
```bash
# Endpoints API (parall√®le - fichiers diff√©rents)
T√¢che T017: Impl√©mentation des endpoints API Website [P]
T√¢che T018: Impl√©mentation des endpoints API Metrics [P]
T√¢che T019: Impl√©mentation des endpoints API ScrapingSession [P]
T√¢che T020: Impl√©mentation de l'endpoint API de v√©rification de sant√© [P]
```

### Phase 4: Int√©gration (S√©quentielle)
```bash
# Services d'int√©gration (s√©quentiel √† cause des d√©pendances)
T√¢che T021: Int√©gration du client API MyToolsPlan
T√¢che T022: Service d'int√©gration de base de donn√©es
T√¢che T023: Int√©gration du logging et monitoring
```

### Phase 5: Finition (Parall√®le)
```bash
# T√¢ches de finition (parall√®le - fichiers diff√©rents)
T√¢che T024: Tests unitaires pour tous les services [P]
T√¢che T025: Optimisation des performances [P]
T√¢che T026: Documentation et documentation API [P]
T√¢che T027: Validation end-to-end [P]
T√¢che T028: Pr√©paration du d√©ploiement en production
```

## Task Dependencies Summary

**Chemin Critique**: T001 ‚Üí T002 ‚Üí T003 ‚Üí T010 ‚Üí T013 ‚Üí T014 ‚Üí T021 ‚Üí T022 ‚Üí T023 ‚Üí T028

**Opportunit√©s Parall√®les**: 
- Tests de contrats (T003-T009) peuvent s'ex√©cuter en parall√®le apr√®s T002
- Endpoints API (T017-T020) peuvent s'ex√©cuter en parall√®le apr√®s les entit√©s core
- T√¢ches de finition (T024-T027) peuvent s'ex√©cuter en parall√®le apr√®s l'int√©gration

**Timeline Estim√©**: 
- Configuration et Tests: 2-3 jours
- Impl√©mentation Core: 5-7 jours  
- Impl√©mentation API: 2-3 jours
- Int√©gration: 3-4 jours
- Finition: 2-3 jours
- **Total**: 14-20 jours

---

## üèóÔ∏è MIGRATION TECHNIQUE (PRIORIT√â BASSE)

### Phase 1: Migration des Extractors (1-2 semaines)
```bash
# Migration progressive des extractors JavaScript vers Python
T058: Migration de BaseExtractor vers Python - base_extractor.py
T059: Migration de TrendTrackExtractor vers Python - trendtrack_extractor.py
T060: Migration des s√©lecteurs vers Python - selectors.py
T061: Tests de validation des extractors Python vs JavaScript
T062: Optimisation des performances des extractors Python
```

### Phase 2: Migration de l'Orchestrateur (1 semaine)
```bash
# Migration compl√®te du scraper principal
T063: Migration de TrendTrackScraper vers Python - trendtrack_scraper.py
T064: Migration de WebScraper vers Python - web_scraper.py
T065: Migration de ErrorHandler vers Python - error_handler.py
T066: Tests d'int√©gration complets du syst√®me Python
T067: Validation des performances du syst√®me Python
```

### Phase 3: Finalisation et Nettoyage (3-5 jours)
```bash
# Suppression du code JavaScript legacy
T068: Suppression du code JavaScript legacy
T069: Mise √† jour de la documentation technique
T070: Tests de r√©gression finaux
T071: Optimisation finale des performances
T072: Documentation de l'architecture Python finale
```

### Notes sur la Migration
- **Priorit√©**: Basse (apr√®s stabilisation de l'approche hybride)
- **Strat√©gie**: Migration progressive avec tests de validation
- **Rollback**: Possible √† chaque √©tape
- **Architecture hybride**: Maintenue jusqu'√† migration compl√®te

---

*T√¢ches g√©n√©r√©es: 2025-09-16*
*Pr√™t pour l'impl√©mentation suivant les principes constitutionnels*

